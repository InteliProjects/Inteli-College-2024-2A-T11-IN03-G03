{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração de Dados, Pré-processamento e Levantamento de Hipóteses\n",
    "\n",
    "# Seção 4.2 (SPRINT 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook documenta a análise e transformação dos dados realizada durante a sprint. Serão apresentados os processos de exploração, pré-processamento dos dados e as hipóteses geradas a partir da análise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1. Exploração de Dados (EDA)\n",
    "\n",
    "Nesta seção, será apresentada uma análise detalhada das características das variáveis que compõem o conjunto de dados. O objetivo é identificar as principais tendências, padrões e possíveis inconsistências, facilitando o entendimento do comportamento dos dados. Esta análise é crucial para direcionar as etapas subsequentes do desenvolvimento do modelo preditivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalando bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fazendo o merge das tabelas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "generate_plots = True  # Mude caso queira regerar todos os gráficos (pode demorar)\n",
    "\n",
    "# Definindo os caminhos dos arquivos\n",
    "file_paths = [\n",
    "    \"../assets/dataset/month_2.csv\",\n",
    "    \"../assets/dataset/month_3.csv\",\n",
    "    \"../assets/dataset/month_4.csv\",\n",
    "    \"../assets/dataset/month_5.csv\",\n",
    "    \"../assets/dataset/month_6.csv\",\n",
    "]\n",
    "\n",
    "info_cadastral = pd.read_csv(\"../assets/dataset/informacao_cadastral.csv\")\n",
    "\n",
    "\n",
    "# Carregando os arquivos em DataFrames\n",
    "dfs = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Junta todos os DataFrames em um único DataFrame por 'clientCode' e 'clientIndex'\n",
    "df_mensal = pd.concat(dfs, ignore_index=True)\n",
    "df = pd.merge(df_mensal, info_cadastral, on=[\"clientCode\", \"clientIndex\"], how=\"left\")\n",
    "\n",
    "# Exibe as primeiras 5 linhas do DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Colunas no dataframe (df): \\n\\n{df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estatística Descritiva\n",
    "\n",
    "A seguir, são apresentadas as estatísticas descritivas básicas de cada coluna do conjunto de dados, bem como a classificação das variáveis em numéricas ou categóricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descrição Básica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra o tamanho e a forma do DataFrame\n",
    "print(\n",
    "    f\"Formato do DataFrame (linha, colunas): \\nLinhas: {df.shape[0]} \\nColunas: {df.shape[1]}\"\n",
    ")\n",
    "\n",
    "# Estatísticas descritivas do DataFrame (média, desvio padrão, mínimo, máximo, etc)\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "df.describe().apply(lambda x: x.apply(\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando valores nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcula a porcentagem de valores ausentes em cada coluna\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Ordena as colunas com valores ausentes em ordem decrescente\n",
    "missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Cria um gráfico de barras com as porcentagens de valores ausentes\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = missing_percentage.plot(kind=\"bar\", color=\"salmon\")\n",
    "plt.title(\"Percentual de Valores Ausentes por Coluna\")\n",
    "plt.xlabel(\"Colunas\")\n",
    "plt.ylabel(\"Percentual de Valores Ausentes\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adiciona os números de valores nulos na diagonal\n",
    "for bar, missing_count in zip(bars.patches, missing_percentage):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height(),\n",
    "        f\"{missing_count:.2f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        rotation=45,\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colunas de geolocalização ('gatewayGeoLocation.alt', 'gatewayGeoLocation.lat' e 'gatewayGeoLocation.long') tem quase 80% de nulos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de dados cada coluna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as colunas entre identificadoras, categóricas e numéricas\n",
    "id_features = [\"clientCode\", \"clientIndex\", \"condCode\", \"condIndex\", \"meterSN\"]\n",
    "categorical_features = [\n",
    "    col\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns\n",
    "    if col not in id_features\n",
    "]\n",
    "numerical_features = [\n",
    "    col\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    if col not in id_features\n",
    "]\n",
    "\n",
    "# Exibindo as colunas categóricas e numéricas\n",
    "print(f\"{len(id_features)} id features:\", id_features)\n",
    "print(f\"{len(categorical_features)} categorical features:\", categorical_features)\n",
    "print(f\"{len(numerical_features)} numerical features:\", numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando Variáveis Chave e Relações\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de Correlação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcula a matriz de correlação\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Cria um gráfico de calor com a matriz de correlação\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de Dispersão de meterIndex vs. initialIndex com gain como Escala de Cor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = sns.scatterplot(data=df, x='meterIndex', y='initialIndex', hue='gain', palette='viridis', s=100)\n",
    "plt.title('Gráfico de Dispersão de meterIndex vs. initialIndex com gain como Escala de Cor')\n",
    "plt.xlabel('Meter Index')\n",
    "plt.ylabel('Initial Index')\n",
    "\n",
    "# Adiciona a legenda ao gráfico\n",
    "scatter.legend(title='Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograma de RSSI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"rssi\"], kde=True, bins=20, color=\"dodgerblue\")\n",
    "plt.title(\"Distribuição de RSSI\")\n",
    "plt.xlabel(\"RSSI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot de Gain e Pulse Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x=\"gain\", y=\"pulseCount\")\n",
    "plt.title(\"Scatter Plot de Gain vs. Pulse Count\")\n",
    "plt.xlabel(\"Gain\")\n",
    "plt.ylabel(\"Pulse Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain aparenta só ter valores de 0.1 e 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gain\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot de meterIndex vs. pulseCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um gráfico de barras com a contagem de valores únicos da coluna 'gain'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df[\"pulseCount\"], df[\"meterIndex\"], alpha=0.5, color=\"blue\")\n",
    "\n",
    "# Adiciona título e rótulos aos eixos\n",
    "plt.title(\"Scatter Plot of Pulse Count vs. Meter Index\")\n",
    "plt.xlabel(\"Pulse Count\")\n",
    "plt.ylabel(\"Meter Index\")\n",
    "\n",
    "# Exibe o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograma de pulseCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"pulseCount\"], bins=5, kde=False, color=\"teal\")\n",
    "plt.title(\"Histogram of Pulse Count\")\n",
    "plt.xlabel(\"Pulse Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pulseCount\"].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograma de todas as colunas numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria histogramas para todas as colunas numéricas\n",
    "df.hist(bins=5, figsize=(30, 30), color=\"teal\")\n",
    "plt.suptitle(\"Distribuição de colunas numéricas\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de Regressão de pulseCount vs. rssi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(data=df, x='pulseCount', y='rssi', ci=95, scatter_kws={'color':'blue'}, line_kws={'color':'red'})\n",
    "    plt.title('Gráfico de Regressão de pulseCount vs. RSSI')\n",
    "    plt.xlabel('Pulse Count')\n",
    "    plt.ylabel('RSSI')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de regressão de pulseCount vs. meterIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_plots:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(data=df, x='pulseCount', y='meterIndex', ci=95, scatter_kws={'color':'blue'}, line_kws={'color':'red'})\n",
    "    plt.title('Gráfico de Regressão de pulseCount vs. meterIndex')\n",
    "    plt.xlabel('Pulse Count')\n",
    "    plt.ylabel('meterIndex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. Pré-processamento dos Dados\n",
    "\n",
    "O pré-processamento dos dados envolve a limpeza e transformação das variáveis para preparar o conjunto de dados para a modelagem. Nesta seção, serão abordadas as seguintes etapas: tratamento de valores ausentes, identificação de outliers e criação de novas features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de Dados\n",
    "\n",
    "Para melhorar a performance dos modelos preditivos, algumas variáveis foram transformadas. A seguir estão as transformações aplicadas:\n",
    "\n",
    "- **Normalização**: A variável `gain` foi normalizada para reduzir o impacto de escalas diferentes entre as variáveis.\n",
    "- **Transformação logarítmica**: Aplicada na variável `pulseCount` para lidar com a alta variabilidade e melhorar a distribuição dos dados.\n",
    "- **Codificação de variáveis categóricas**: As variáveis `inputType` e `model` foram codificadas usando one-hot encoding para permitir que os modelos utilizem essas informações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de valores nulos\n",
    "\n",
    "De acordo com o gráfico de percentual de valores ausentes por coluna e a tabela de estatística descritiva, podemos concluir que os valores nulos nas colunas 'gain' e 'rssi', poderão ser substituidos por suas respectivas média, moda ou mediana, sem afetar significativamente qualquer análise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratando o rssi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi_mean = df[\"rssi\"].mean()  # Calcula a média da coluna rssi\n",
    "rssi_mode = df[\"rssi\"].mode()  # Calcula a moda da coluna rssi\n",
    "rssi_median = df[\"rssi\"].median()  # Calcula a mediana da coluna rssi\n",
    "\n",
    "print(f\"Média da coluna rssi: {rssi_mean:.2f}\")\n",
    "print(f\"Moda da coluna rssi: {rssi_mode.values[0]:.2f}\")\n",
    "print(f\"Mediana da coluna rssi: {rssi_median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rssi\"] = df[\"rssi\"].replace(\n",
    "    np.nan, rssi_mean\n",
    ")  # Substitui os valores nulos pela média da coluna rssi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratando o gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"gain\"] = df[\"gain\"].replace(\n",
    "    np.nan, 1\n",
    ")  # Substitui os valores nulos pela moda da coluna gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Categóricos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar a cardinalidade de cada feature categórica\n",
    "cardinalities = {}\n",
    "\n",
    "# Loop através de cada feature categórica, excluindo 'datetime'\n",
    "for feature in categorical_features:\n",
    "    if feature != \"datetime\":  # Exclui a coluna 'datetime'\n",
    "        cardinality = len(df[feature].unique())\n",
    "        cardinalities[feature] = cardinality\n",
    "\n",
    "# Ordena o dicionário pela cardinalidade em ordem decrescente\n",
    "sorted_cardinalities = dict(\n",
    "    sorted(cardinalities.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "# Imprime a cardinalidade de cada feature em ordem decrescente\n",
    "for feature, cardinality in sorted_cardinalities.items():\n",
    "    print(f\"Feature: {feature}, Cardinalidade: {cardinality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime a cardinalidade e os primeiros 10 valores únicos de cada feature em ordem decrescente\n",
    "for feature in sorted_cardinalities:\n",
    "    unique_values = df[feature].unique()\n",
    "    cardinality = sorted_cardinalities[feature]\n",
    "\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"Cardinalidade: {cardinality}\")\n",
    "    print(f\"Primeiros 10 Valores Únicos: {unique_values[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"perfil_consumo\"].value_counts()  # Mostra os perfis de consumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que, na verdade, há apenas 3 categorias em perfil de consumo: Aquecedor, Cocção, e Piscina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contratação, CEP e bairro possuem alta cardinalidades vamos ter que tratar de alguma forma diferente.\n",
    "\n",
    "Para a contratação podemos criar uma feature de dias desde a contratação.\n",
    "\n",
    "Para o CEP podemos remover o '-' e tratar como número, uma vez que números próximos de CEP possuem também proximidade geográfia (i.e. o CEP '22000-200' é mais próximo geográficamente de '23200-200' do que de '12200-200').\n",
    "Já para o bairro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para bairro podemos analisar o gráfico, parece fazer sentido pegar as 9 categorias mais frequentes (até Santa Cecília), pois com 1/3 da cardinalidade (9 comparado ao total de 27 bairros) corresponde a mais de 80% dos bairros.\n",
    "Assim transformaremos os demais bairros em uma coluna de \"outros\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entedimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colunas apresentam baixa cardinalidade, a maior sendo 'inputType' com 9 categorias diferentes, e nenhuma sendo ordinal.\n",
    "Assim usaremos OneHotEncoding (OHE) em todas elas (exceto 'perfil_consumo', 'CEP', 'bairro' e 'contratacao'), a fim de preservar ao máximo a informação contida nas variáveis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do perfil de consumo ('perfil_consumo'), é possível perceber que existem apenas 3 categorias reais (Aquecedor, Cocção e Piscina), assim criaremos 3 colunas binárias, onde 1 indicará que o cliente possui, mas não necessariamente de forma exclusiva, aquele perfil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando df_encoded para preservar o df original\n",
    "df_encoded = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformação da data para o formato unix timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"datetime\"] = pd.to_datetime(\n",
    "    df[\"datetime\"]\n",
    ")  # Converte a coluna datetime para o formato datetime\n",
    "\n",
    "df[\"timestamp\"] = df[\"datetime\"].apply(\n",
    "    lambda x: x.timestamp() if pd.notnull(x) else None\n",
    ")  # Cria uma nova coluna derivada da coluna datetime no DataFrame com os valores convertidos para timestamp, ignorando os valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enconding Geral (OneHotEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover as colunas que terão um tratamento diferente\n",
    "ohe_categorical_cols = [\n",
    "    col\n",
    "    for col in categorical_features\n",
    "    if col not in [\"datetime\", \"perfil_consumo\", \"cep\", \"bairro\", \"contratacao\"]\n",
    "]\n",
    "\n",
    "# Aplicar OneHotEncoder nas colunas selecionadas, mantendo o prefixo da variável original\n",
    "df_encoded_ohe = pd.get_dummies(\n",
    "    df_encoded[ohe_categorical_cols], prefix=ohe_categorical_cols\n",
    ")\n",
    "\n",
    "# Contar quantas colunas foram geradas no processo de One-Hot Encoding\n",
    "num_new_columns = df_encoded_ohe.shape[1]\n",
    "num_initial_columns = len(ohe_categorical_cols)\n",
    "num_total_columns_before = df_encoded.shape[1]\n",
    "num_total_columns_after = df_encoded.shape[1]\n",
    "\n",
    "# Exibir informações detalhadas sobre o processo de codificação\n",
    "print(f\"Quantidade inicial de colunas categóricas codificadas: {num_initial_columns}\")\n",
    "print(f\"Número de novas colunas geradas pelo One-Hot Encoding: {num_new_columns}\")\n",
    "print(f\"Número total de colunas antes da codificação (df): {num_total_columns_before}\")\n",
    "print(\n",
    "    f\"Número total de colunas após a codificação (df_encoded): {num_total_columns_after}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Exibir as primeiras linhas das novas colunas geradas\n",
    "print(\"\\nVisualização das novas colunas geradas:\")\n",
    "print(df_encoded_ohe.head())\n",
    "\n",
    "# Manter as colunas que não foram codificadas\n",
    "df_remaining = df_encoded.drop(columns=ohe_categorical_cols)\n",
    "\n",
    "# Concatenar as colunas codificadas com as restantes do DataFrame\n",
    "df_encoded = pd.concat([df_remaining, df_encoded_ohe], axis=1)\n",
    "\n",
    "# Verificação final: Exibir as primeiras linhas do DataFrame completo\n",
    "print(\"Primeiras linhas do DataFrame completo com todas as colunas:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding 'contratação'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o número de valores NaN em 'contratacao'\n",
    "num_nan_contratacao = df_encoded[\"contratacao\"].isna().sum()\n",
    "\n",
    "# Calcular o percentual de NaN em relação ao número total de linhas\n",
    "percentual_nan_contratacao = (num_nan_contratacao / len(df_encoded)) * 100\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Número de valores NaN em 'contratacao': {num_nan_contratacao}\")\n",
    "print(f\"Percentual de valores NaN em 'contratacao': {percentual_nan_contratacao:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar o encoding de 'contratacao', vamos transformar a data de contratação em uma feature nova: número de dias desde a contrataçao ('dias_desde_contratacao'), tendo como data de referência o último dado disponível na base de dados, e tratando-a como 'int' daqui pra frente.\n",
    "\n",
    "Depois, iremos inputar os dados nulos pela mediana e criar uma coluna binária 'contratacao_isnan' que persiste a inforamção de quais linhas eram nulas nessa coluna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as colunas 'contratacao' e 'datetime' para formato datetime\n",
    "df_encoded[\"contratacao\"] = pd.to_datetime(df_encoded[\"contratacao\"], errors=\"coerce\")\n",
    "df_encoded[\"datetime\"] = pd.to_datetime(df_encoded[\"datetime\"], errors=\"coerce\")\n",
    "\n",
    "# Encontrar o último dia de contratação (ignorando NaNs)\n",
    "ultimo_dia_contratacao = df_encoded[\"contratacao\"].max()\n",
    "\n",
    "# Encontrar o último timestamp (ignorando NaNs)\n",
    "ultimo_timestamp = df_encoded[\"datetime\"].max()\n",
    "\n",
    "# Exibir as datas encontradas\n",
    "print(f\"Último dia de contratação: {ultimo_dia_contratacao}\")\n",
    "print(f\"Último datetime: {ultimo_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o último 'datetime' (variável 'ultimo_timestamp') como data de referência para nossa feature!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Converter a coluna 'contratacao' para o formato datetime\n",
    "df_encoded[\"contratacao\"] = pd.to_datetime(df_encoded[\"contratacao\"], errors=\"coerce\")\n",
    "\n",
    "# Passo 2: Calcular o número de dias até a data de referência\n",
    "data_referencia = ultimo_timestamp  # Último timestamp encontrado\n",
    "df_encoded[\"dias_ate_referencia\"] = (\n",
    "    data_referencia - df_encoded[\"contratacao\"]\n",
    ").dt.days\n",
    "\n",
    "# Passo 3: Criar a coluna binária 'contratacao_nan' para indicar valores NaN originais\n",
    "df_encoded[\"contratacao_nan\"] = df_encoded[\"dias_ate_referencia\"].isna().astype(int)\n",
    "\n",
    "# Passo 4: Substituir os valores NaN pela mediana\n",
    "mediana_dias = df_encoded[\"dias_ate_referencia\"].median()\n",
    "df_encoded[\"dias_ate_referencia\"] = df_encoded[\"dias_ate_referencia\"].fillna(\n",
    "    mediana_dias\n",
    ")\n",
    "\n",
    "# Verificar a nova feature e a coluna binária\n",
    "print(df_encoded[[\"contratacao\", \"dias_ate_referencia\", \"contratacao_nan\"]].head())\n",
    "\n",
    "# Remover a coluna original 'contratacao'\n",
    "df_encoded.drop(columns=[\"contratacao\"], inplace=True)\n",
    "\n",
    "# Contar o número de valores 1 na coluna 'contratacao_nan'\n",
    "num_contratacao_nan_ones = df_encoded[\"contratacao_nan\"].sum()\n",
    "print(f\"\\nNúmero de valores 1 na coluna 'contratacao_nan': {num_contratacao_nan_ones}\")\n",
    "print(\n",
    "    f\"Percentual de valores NaN em 'contratacao_nan': {num_contratacao_nan_ones/len(df_encoded):.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding 'cep'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o número de valores NaN em 'cep'\n",
    "num_nan_cep = df_encoded[\"cep\"].isna().sum()\n",
    "\n",
    "# Calcular o percentual de NaN em relação ao número total de linhas\n",
    "percentual_num_nan_cep = (num_nan_cep / len(df_encoded)) * 100\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Número de valores NaN em 'cep' pré-encoding: {num_nan_cep}\")\n",
    "print(f\"Percentual de valores NaN em 'cep' pré-encoding: {percentual_num_nan_cep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para 'cep' vamos substituir os valores 'nan' por 0, após isso retirar o hífen e tratar os valores como 'int', conforme a explicação na seção de entendimento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Remover hífens e espaços em branco do CEP\n",
    "df_encoded[\"cep_cleaned\"] = df_encoded[\"cep\"].str.replace(\"-\", \"\").str.strip()\n",
    "\n",
    "# Passo 2: Substituir strings vazias ou espaços em branco por NaN\n",
    "df_encoded[\"cep_cleaned\"] = df_encoded[\"cep_cleaned\"].replace(\"\", float(\"nan\"))\n",
    "\n",
    "# Passo 3: Substituir NaNs por um valor adequado, por exemplo, 0\n",
    "df_encoded[\"cep_cleaned\"] = df_encoded[\"cep_cleaned\"].fillna(\"0\")\n",
    "\n",
    "# Passo 4: Converter a coluna limpa para inteiro\n",
    "df_encoded[\"cep\"] = df_encoded[\"cep_cleaned\"].astype(int)\n",
    "\n",
    "# Passo 5: Remover a coluna 'cep_cleaned'\n",
    "df_encoded = df_encoded.drop(columns=[\"cep_cleaned\"])\n",
    "\n",
    "# Verificar as primeiras linhas para garantir que a conversão foi bem-sucedida\n",
    "print(df_encoded[\"cep\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de ocorrências onde o CEP é igual a 0\n",
    "count_cep_zero = (df_encoded[\"cep\"] == 0).sum()\n",
    "\n",
    "# Calcular o percentual que isso representa em relação ao total de linhas\n",
    "percent_cep_zero = (count_cep_zero / len(df_encoded)) * 100\n",
    "\n",
    "print(f\"Contagem de valores onde o CEP é igual a 0 pós-encoding: {count_cep_zero}\")\n",
    "print(\n",
    "    f\"Percentual de valores onde o CEP é igual a 0 pós-encoding: {percent_cep_zero:.2f}%\"\n",
    ")\n",
    "\n",
    "print(f\"\\nNúmero de valores NaN em 'cep' pré-encoding: {num_nan_cep}\")\n",
    "print(f\"Percentual de valores NaN em 'cep' pré-encoding: {percentual_num_nan_cep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos o encoding, mas algo não parece certo... apareceram cerca de 2 mil linhas a mais como zero em relação ao número de NaNs inicial em 'cep'.\n",
    "\n",
    "Vamos investigar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover espaços em branco e verificar se os valores são apenas '-' ou strings vazias após o strip\n",
    "df[\"cep_cleaned\"] = df[\"cep\"].str.strip()\n",
    "\n",
    "# Contar o número de ocorrências onde o CEP é apenas '-' ou uma string vazia\n",
    "count_hyphen_or_empty = ((df[\"cep_cleaned\"] == \"-\") | (df[\"cep_cleaned\"] == \"\")).sum()\n",
    "\n",
    "print(\n",
    "    f\"Número de linhas em df onde o CEP é apenas '-' ou uma string vazia após o strip: {count_hyphen_or_empty}\"\n",
    ")\n",
    "print(f\"Diferença não explicada pré e pós encoding: {count_cep_zero - num_nan_cep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos justificar. Os casos eram preenchimentos que estavam somente com espaços ou hífen, assim, após o 'strip()' (que remove espaços) podemos ver que eram, na verdade, células não-preenchidas.\n",
    "\n",
    "Podemos remover o df['cep_cleand'] agora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"cep_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding 'bairro'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"bairro\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o número de valores NaN em 'bairro'\n",
    "num_nan_bairro = df_encoded[\"bairro\"].isna().sum()\n",
    "\n",
    "# Calcular o percentual de NaN em relação ao número total de linhas\n",
    "percentual_num_nan_bairro = (num_nan_bairro / len(df_encoded)) * 100\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"Número de valores NaN em 'bairro' pré-encoding: {num_nan_bairro}\")\n",
    "print(\n",
    "    f\"Percentual de valores NaN em 'bairro' pré-encoding: {percentual_num_nan_bairro:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Calcular a frequência de cada categoria na coluna 'bairro' e ordenar\n",
    "frequencia_bairro = df_encoded[\"bairro\"].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Calcular a frequência acumulada\n",
    "frequencia_acumulada = frequencia_bairro.cumsum()\n",
    "\n",
    "# Normalizar a frequência acumulada para obter a proporção (entre 0 e 1)\n",
    "frequencia_acumulada = frequencia_acumulada / frequencia_acumulada.max()\n",
    "\n",
    "# Passo 2: Plotar a frequência acumulada\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(frequencia_acumulada, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "plt.title('Frequência Acumulada de \"Bairro\" do Maior para o Menor')\n",
    "plt.xlabel(\"Bairro\")\n",
    "plt.ylabel(\"Frequência Acumulada (Normalizada)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para bairro podemos analisar o gráfico, parece fazer sentido pegar as 9 categorias mais frequentes (até Santa Cecília), pois com 1/3 da cardinalidade (9 comparado ao total de 27 bairros) corresponde a mais de 80% dos bairros.\n",
    "Assim transformaremos os demais bairros em uma coluna de \"outros\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os 9 principais bairros\n",
    "top_9_bairros = frequencia_bairro.head(9).index\n",
    "\n",
    "# Passo 2: Substituir os bairros que não estão entre os 9 principais por \"Outros\"\n",
    "df_encoded[\"bairro\"] = df_encoded[\"bairro\"].apply(\n",
    "    lambda x: x if x in top_9_bairros else \"Outros\"\n",
    ")\n",
    "\n",
    "# Passo 3: Verificar o resultado\n",
    "print(df_encoded[\"bairro\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar o número de ocorrências de \"Outros\"\n",
    "outros_count = df_encoded[\"bairro\"].value_counts()[\"Outros\"]\n",
    "\n",
    "# Contar o total de registros\n",
    "total_count = len(df_encoded)\n",
    "\n",
    "# Calcular o percentual de \"Outros\" em relação ao total\n",
    "percent_outros = (outros_count / total_count) * 100\n",
    "\n",
    "# Imprimir o resultado\n",
    "print(f\"Número de registros como 'Outros': {outros_count}\")\n",
    "print(f\"Percentual de 'Outros' em relação ao total: {percent_outros:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Outros' é 20% dos casos, como queríamos, a diferença em relação ao gráfico é por causa dos 3.97% de casos nulos. Esses casos não serão 'True' (1) em nenhuma das colunas binárias após o OHE, assim, preservaremos a informação de queram nulos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora só precisamos fazer o OHE das categorias restantes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Aplicar o One-Hot Encoding na coluna 'bairro'\n",
    "bairro_dummies = pd.get_dummies(df_encoded[\"bairro\"], prefix=\"bairro\")\n",
    "\n",
    "# Passo 2: Remover a coluna original 'bairro'\n",
    "df_encoded = df_encoded.drop(columns=[\"bairro\"])\n",
    "\n",
    "# Passo 3: Adicionar as colunas codificadas ao DataFrame original\n",
    "df_encoded = pd.concat([df_encoded, bairro_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos exibir todas as colunas que começam com 'bairro_' para ver como ficou a codificação\n",
    "print(f\"Número de Colunas: {len(df_encoded.filter(like='bairro_').columns)}\")\n",
    "print(f\"Colunas: {df_encoded.filter(like='bairro_').columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar quantas colunas foram geradas no processo de One-Hot Encoding\n",
    "num_total_columns_before = df_encoded.shape[1]\n",
    "num_total_columns_after = df_encoded.shape[1]\n",
    "\n",
    "# Exibir informações detalhadas sobre o processo de codificação\n",
    "print(f\"Número total de colunas antes da codificação (df): {num_total_columns_before}\")\n",
    "print(\n",
    "    f\"Número total de colunas após a codificação (df_encoded): {num_total_columns_after}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding 'perfil_consumo'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relembrando as categorias de 'perfil_consumo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"perfil_consumo\"\n",
    "unique_values = df_encoded[feature].unique()\n",
    "cardinality = sorted_cardinalities[feature]\n",
    "\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Cardinalidade: {cardinality}\")\n",
    "print(f\"Primeiros 10 Valores Únicos: {unique_values[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Criar as colunas binárias para cada perfil de consumo\n",
    "df_encoded[\"perfil_aquecedor\"] = df_encoded[\"perfil_consumo\"].apply(\n",
    "    lambda x: 1 if \"Aquecedor\" in str(x) else 0\n",
    ")\n",
    "df_encoded[\"perfil_coccao\"] = df_encoded[\"perfil_consumo\"].apply(\n",
    "    lambda x: 1 if \"Cocção\" in str(x) else 0\n",
    ")\n",
    "df_encoded[\"perfil_piscina\"] = df_encoded[\"perfil_consumo\"].apply(\n",
    "    lambda x: 1 if \"Piscina\" in str(x) else 0\n",
    ")\n",
    "\n",
    "# Passo 2: Verificar o resultado\n",
    "print(\n",
    "    df_encoded[\n",
    "        [\"perfil_consumo\", \"perfil_aquecedor\", \"perfil_coccao\", \"perfil_piscina\"]\n",
    "    ].sample(20)\n",
    ")\n",
    "\n",
    "# Passo 3: Remover a coluna original 'perfil_consumo'\n",
    "df_encoded = df_encoded.drop(columns=[\"perfil_consumo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: não precisamos alterar os valores 'nan' ou '-', vistos nos valores únicos printados acima, porque esses valores serão codificados como 'False' nas 3 colunas binárias criadas para 'perfil_consumo'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular os percentuais de True (1) em cada coluna de perfil\n",
    "percentuais_perfil = (\n",
    "    df_encoded[[\"perfil_aquecedor\", \"perfil_coccao\", \"perfil_piscina\"]].mean() * 100\n",
    ")\n",
    "\n",
    "# Exibir os percentuais\n",
    "print(\"Percentuais de True (1) em cada coluna de perfil:\")\n",
    "print(percentuais_perfil)\n",
    "\n",
    "# Filtrar os casos onde todos os 3 perfis são False (0)\n",
    "casos_sem_perfil = df_encoded[\n",
    "    (df_encoded[\"perfil_aquecedor\"] == 0)\n",
    "    & (df_encoded[\"perfil_coccao\"] == 0)\n",
    "    & (df_encoded[\"perfil_piscina\"] == 0)\n",
    "]\n",
    "\n",
    "# Calcular e exibir o percentual de casos onde todos os perfis são False\n",
    "percentual_sem_perfil = (casos_sem_perfil.shape[0] / len(df_encoded)) * 100\n",
    "print(\n",
    "    f\"\\nPercentual de casos onde todos os perfis são False (0): {percentual_sem_perfil:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis Numéricas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[numerical_features].describe().apply(\n",
    "    lambda x: x.apply(\"{:.2f}\".format)\n",
    ")  # Mostra as estatisticas descritivas das colunas numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha as estatísticas descritivas\n",
    "stats = df_encoded[numerical_features].describe()\n",
    "\n",
    "# Calcule a razão do desvio-padrão sobre a média para cada coluna numérica\n",
    "std_dev_to_mean_ratio = stats.loc[\"std\"] / stats.loc[\"mean\"]\n",
    "\n",
    "print(std_dev_to_mean_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que das variáveis numéricas iniciais (antes da codificação das variáveis categóricas) a 'pulseCount' se sobressai pelo seu alto desvio-padrão, por isso aplicaremos uma transformação logarítmica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformação logarítmica da coluna pulseCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transformação logarítmica na coluna pulseCount e salvar no df_encoded\n",
    "df_encoded[\"log_pulseCount\"] = np.log1p(\n",
    "    df_encoded[\"pulseCount\"]\n",
    ")  # Aplica a transformação logarítmica na coluna pulseCount\n",
    "\n",
    "# Verifica algumas linhas aleatórias para garantir que a transformação foi bem-sucedida\n",
    "print(df_encoded[[\"pulseCount\", \"log_pulseCount\"]].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalização da Variável Gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização da variável gain\n",
    "df_encoded[\"gain\"] = (df_encoded[\"gain\"] - df_encoded[\"gain\"].mean()) / df_encoded[\n",
    "    \"gain\"\n",
    "].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entendimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém os tipos de dados das colunas\n",
    "dtypes = df_encoded.dtypes\n",
    "\n",
    "# Cria listas de colunas com base no dtype\n",
    "object_cols = [col for col in dtypes.index if dtypes[col] == \"object\"]\n",
    "non_object_cols = [col for col in dtypes.index if dtypes[col] != \"object\"]\n",
    "\n",
    "# Cria a lista de colunas ordenada\n",
    "sorted_cols = object_cols + non_object_cols\n",
    "\n",
    "# Reordena o DataFrame conforme a lista ordenada\n",
    "df_sorted = df_encoded[sorted_cols]\n",
    "\n",
    "print(\"Informações do dataframe codificado (df_encoded) ordenado pelo data type:\\n\")\n",
    "print(df_sorted.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que as únicas colunas que não são númericas são as colunas identificadores ('id_features'), assim nossa codificação está completa!\n",
    "\n",
    "Nota: poderíamos utilizar um escalador (como o MinMaxScaler do Sklearn) nas colunas a partir de agora, mas escolhemos preservar a escala dos dados para manter a interpretabilidade e evitar limitações para a próxima etapa de feature engineering e criação de modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste projeto, os outliers não serão removidos, pois o foco é identificar anomalias no consumo de gás. Em vez disso, estamos tratando os outliers como casos de interesse que precisam ser analisados e modelados adequadamente. A seguir, são apresentadas as técnicas utilizadas para a detecção e análise desses outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualização de Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos outliers em meterIndex e pulseCount\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[['meterIndex', 'pulseCount']])\n",
    "plt.title('Visualização dos Outliers em meterIndex e pulseCount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Preparação dos Dados e Modelagem\n",
    "\n",
    "Nesta seção, será criada a primeira versão do modelo preditivo, as escolhas de features e resultados serão explicados e analisados.\n",
    "\n",
    "### 4.3.1. Modelo Não-Supervisionado - Modelagem, features e explicação\n",
    "\n",
    "_Hipotese inicial_\n",
    "Partimos da premissa de que dispúnhamos de uma quantidade suficiente de CEPs para realizar comparações significativas. Acreditávamos que indivíduos que moram próximos uns dos outros apresentariam padrões de consumo semelhantes. Nossa abordagem consistia em avaliar a variação da taxa de consumo ao longo de uma semana e, em seguida, comparar esses dados com indivíduos agrupados em clusters específicos.\n",
    "\n",
    "No entanto, enfrentamos um problema: a quantidade de CEPs disponível não foi suficiente para realizar uma análise robusta. Como resultado, não conseguimos formar clusters que fossem suficientemente representativos para validar nossa hipótese.\n",
    "\n",
    "_Nova Abordagem:_\n",
    "\n",
    "Dado que decidimos não seguir com a hipótese inicial, adotamos uma nova estratégia para análise. Nossa abordagem agora foca na comparação entre o consumo individual e o comportamento médio/mediano da residência por dia da semana. A seguir, detalhamos os principais passos e considerações:\n",
    "\n",
    "Análise Residência vs. Indivíduo por Dia da Semana:\n",
    "\n",
    "O consumo de cada indivíduo será comparado com a média ou mediana de consumo do prédio (condomínio) onde reside.\n",
    "A partir dessa comparação, os resultados entre os indivíduos serão agregados e comparados com a média ou mediana do consumo de cada agrupamento (condIndex).\n",
    "A renda per capita, inferida pelo condCode, também será considerada como um fator influente.\n",
    "Cada grupo de indivíduos dentro de um mesmo condomínio (condCode) será tratado como um cluster. No entanto, é importante notar que nem todos os consumidores estão vinculados a um condomínio.\n",
    "\n",
    "Justificativa da Escolha das Features:\n",
    "\n",
    "pulseCount (Contagem de Pulsos):\n",
    "Esta feature é fundamental, pois representa a base do cálculo de consumo de gás. Cada pulso captado pelo medidor está diretamente relacionado à quantidade de gás consumida. É essencial para medir o consumo de cada cliente de forma precisa e granular.\n",
    "\n",
    "gain (Fator de Multiplicação de Pulsos):\n",
    "O fator de multiplicação de pulsos é necessário para converter a contagem de pulsos em uma unidade mais prática, como metros cúbicos de gás. Essa feature garante que os dados de consumo sejam interpretados corretamente, considerando possíveis diferenças entre medidores.\n",
    "\n",
    "condCode (Código do Condomínio):\n",
    "Essa feature permite identificar e agrupar consumidores que residem em um mesmo condomínio. A análise por agrupamentos, como clusters de consumidores que compartilham o mesmo condCode, facilita a identificação de padrões coletivos e anomalias que podem ser específicas de um determinado condomínio.\n",
    "\n",
    "clientIndex (Identificador Único):\n",
    "O clientIndex é essencial para diferenciar os consumidores individuais. Ele garante que cada unidade de medição seja tratada de forma única, permitindo análises individualizadas mesmo dentro de agrupamentos maiores, como os de condomínio.\n",
    "\n",
    "_pulseCount será multiplicado pelo Gain_\n",
    "\n",
    "Análise Comparativa:\n",
    "\n",
    "Pontos Positivos:\n",
    "Possibilita a identificação de padrões de consumo anômalos com base no comportamento diário.\n",
    "Facilita a segmentação dos consumidores em clusters específicos, como os agrupados por condomínio, com base no consumo diário.\n",
    "Pontos Negativos:\n",
    "Medições coletivas podem influenciar os dados, dificultando a análise individual.\n",
    "A limitação de informações específicas por condomínio pode restringir a precisão da formação dos clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Início do desenvolvimento da abordagem:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação da coluna totalConsumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"totalConsumption\"] = df.pulseCount * df.gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo faremos a validação da hipotese de se é proveitoso ou não agrupar os clusters por condomínio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contagem de quem mais aparece na coluna condCode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condCodeValues = df[\"condCode\"].value_counts()\n",
    "condCodeValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando o condomínio que mais aparece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetCondCode = condCodeValues.index[0]\n",
    "print(targetCondCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o targetCondCodeDf, contendo valores somente desse condomínio que mais aparece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetCondCodeDf = df[df[\"condCode\"] == targetCondCode]\n",
    "targetCondCodeDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Criando a coluna measure diff:_\n",
    "\n",
    "A coluna foi criada organizando o df em ordem crescente, primeiro pelo clientCode depois pelo datetime. depois foi criada uma coluna auxiliar, que depois foi removida, chamada 'match', com valores True se a linha n-1 for do mesmo cliente que a linha n. Se a coluna match for true, a coluna measure_diff calcula a diferença de total consumption da linha n para a linha n-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetCondCodeDf = targetCondCodeDf.sort_values(\n",
    "    by=[\"clientCode\", \"datetime\"], ascending=True\n",
    ")\n",
    "targetCondCodeDf[\"match\"] = df.clientCode.eq(df.clientCode.shift())\n",
    "targetCondCodeDf[\"measure_diff\"] = targetCondCodeDf[\"match\"].where(\n",
    "    targetCondCodeDf[\"match\"] == False, targetCondCodeDf[\"totalConsumption\"].diff()\n",
    ")\n",
    "targetCondCodeDf[\"measure_diff\"] = pd.to_numeric(\n",
    "    targetCondCodeDf[\"measure_diff\"], errors=\"coerce\"\n",
    ")\n",
    "targetCondCodeDf = targetCondCodeDf.dropna(subset=[\"match\"])\n",
    "targetCondCodeDf = targetCondCodeDf.drop(columns=[\"match\"])\n",
    "targetCondCodeDf.reset_index(drop=True, inplace=True)\n",
    "targetCondCodeDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação da coluna measure_avg_consumption agrupando os dados por dia e obtendo a média de measure_diff daquele período.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetCondCodeDf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "targetCondCodeDf[\"datetime_yyyy-mm-dd\"] = targetCondCodeDf[\"datetime\"].dt.normalize()\n",
    "\n",
    "targetCondCodeDf[\"measure_avg_consumption\"] = None\n",
    "\n",
    "targetCondCodeDf = targetCondCodeDf[targetCondCodeDf[\"measure_diff\"] > 0]\n",
    "\n",
    "for date in np.array(targetCondCodeDf[\"datetime_yyyy-mm-dd\"].unique()):\n",
    "    measure_avg_consumption = targetCondCodeDf[\n",
    "        targetCondCodeDf[\"datetime_yyyy-mm-dd\"].dt.to_period(\"D\")\n",
    "        == pd.to_datetime(date).to_period(\"D\")\n",
    "    ]\n",
    "    measure_avg_consumption = measure_avg_consumption[\"measure_diff\"].mean()\n",
    "    targetCondCodeDf.loc[\n",
    "        targetCondCodeDf[\"datetime_yyyy-mm-dd\"] == date, \"measure_avg_consumption\"\n",
    "    ] = measure_avg_consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando os dados de um mês aleatório.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = targetCondCodeDf[\n",
    "    targetCondCodeDf[\"datetime\"].dt.to_period(\"M\")\n",
    "    == pd.to_datetime(\"2024-06\").to_period(\"M\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de média de consumo do condomínio por dia vs usuários.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    filtered_df[\"datetime\"],\n",
    "    filtered_df[\"measure_diff\"],\n",
    "    label=\"Measure Consumption\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    filtered_df[\"datetime\"],\n",
    "    filtered_df[\"measure_avg_consumption\"],\n",
    "    label=\"Average Consumption\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Total vs Average Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando um clientCode aleatório dentro aquele condomínio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_clientCode = targetCondCodeDf[\"clientCode\"].value_counts().index[1]\n",
    "print(target_clientCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico do consumo desse cliente vs a média do condomínio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 6))\n",
    "\n",
    "plt.plot(\n",
    "    filtered_df[filtered_df[\"clientCode\"] == target_clientCode][\"datetime\"],\n",
    "    filtered_df[filtered_df[\"clientCode\"] == target_clientCode][\"measure_diff\"],\n",
    "    label=\"Measure Consumption\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    filtered_df[filtered_df[\"clientCode\"] == target_clientCode][\"datetime\"],\n",
    "    filtered_df[filtered_df[\"clientCode\"] == target_clientCode][\n",
    "        \"measure_avg_consumption\"\n",
    "    ],\n",
    "    label=\"Average Consumption\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Total vs Average Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "# plt.ylim(0, 0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, aplicando a lógica de somente um condomínio de measure_diff para todo o df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"match\"] = df.clientCode.eq(df.clientCode.shift())\n",
    "df[\"measure_diff\"] = df[\"match\"].where(\n",
    "    df[\"match\"] == False, df[\"totalConsumption\"].diff()\n",
    ")\n",
    "df[\"measure_diff\"] = pd.to_numeric(df[\"measure_diff\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"match\"])\n",
    "df = df.drop(columns=[\"match\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, aplicando a lógica de somente um condomínio de measure_avg_consumption para todo o df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df[\"datetime_yyyy-mm-dd\"] = df[\"datetime\"].dt.normalize()\n",
    "\n",
    "df[\"measure_avg_consumption\"] = None\n",
    "\n",
    "for date in np.array(df[\"datetime_yyyy-mm-dd\"].unique()):\n",
    "    measure_avg_consumption = df[\n",
    "        df[\"datetime_yyyy-mm-dd\"].dt.to_period(\"D\")\n",
    "        == pd.to_datetime(date).to_period(\"D\")\n",
    "    ]\n",
    "    measure_avg_consumption = measure_avg_consumption[\"measure_diff\"].mean()\n",
    "    df.loc[df[\"datetime_yyyy-mm-dd\"] == date, \"measure_avg_consumption\"] = (\n",
    "        measure_avg_consumption\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolha das colunas numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    [\"clientCode\", \"condCode\", \"timestamp\", \"measure_avg_consumption\", \"measure_diff\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label enconding de condCode e clientCode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_condCode = LabelEncoder()\n",
    "le_clientCode = LabelEncoder()\n",
    "\n",
    "\n",
    "le_condCode.fit(df[\"condCode\"].dropna())\n",
    "df[\"C_condCode\"] = pd.Series(\n",
    "    le_condCode.transform(df[\"condCode\"].dropna()), index=df[\"condCode\"].dropna().index\n",
    ")\n",
    "df[\"C_condCode\"] = df[\"C_condCode\"].reindex(df.index).astype(float)\n",
    "\n",
    "\n",
    "le_clientCode.fit(df[\"clientCode\"].dropna())\n",
    "df[\"C_clientCode\"] = pd.Series(\n",
    "    le_clientCode.transform(df[\"clientCode\"].dropna()),\n",
    "    index=df[\"clientCode\"].dropna().index,\n",
    ")\n",
    "df[\"C_clientCode\"] = df[\"C_clientCode\"].reindex(df.index).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico da média de consumo de todos os condiminios por dia vs consumo de todos usuários.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df[\"timestamp\"],\n",
    "    df[\"measure_diff\"],\n",
    "    label=\"Measure Consumption\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df[\"timestamp\"],\n",
    "    df[\"measure_avg_consumption\"],\n",
    "    label=\"Average Consumption\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Total vs Average Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0, 20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Modelo Candidato\n",
    "\n",
    "O modelo escolhido é o **KMeans**, um algoritmo de clustering amplamente utilizado para agrupar dados com base em similaridades. Neste caso, foram selecionadas três features relevantes:\n",
    "\n",
    "- **measure_diff**: Diferença de medidas de consumo de gás.\n",
    "- **C_condCode**: Código do condomínio para identificar agrupamentos.\n",
    "- **C_clientCode**: Código do cliente, que representa cada unidade de medição individual.\n",
    "\n",
    "O modelo foi configurado para gerar **3 clusters** e distribuiu os indivíduos com base na similaridade dessas variáveis. Os clusters foram visualizados em um gráfico 3D, com os centróides claramente marcados para indicar os pontos centrais de cada grupo.\n",
    "\n",
    "### Discussão sobre os Resultados\n",
    "\n",
    "O KMeans conseguiu segmentar os consumidores de maneira eficaz, agrupando-os em três clusters distintos. Cada cluster representa um padrão comum de consumo de gás, levando em consideração a localização (condCode) e o comportamento individual (clientCode).\n",
    "\n",
    "- **Os clusters formados** revelam agrupamentos de consumidores com padrões de consumo semelhantes. Isso facilita a análise de comportamento coletivo (dentro de condomínios) e individual.\n",
    "- **Os centróides indicam** os padrões médios de consumo de cada grupo, sendo pontos de referência claros para comparar consumidores dentro do cluster.\n",
    "\n",
    "**Conclusão:**  \n",
    "A escolha do KMeans com 3 clusters forneceu uma segmentação clara e útil dos consumidores, destacando padrões de consumo distintos e criando uma base sólida para análise de comportamento. Essa segmentação permite identificar anomalias ou padrões específicos que podem ser explorados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "X = df[[\"measure_diff\", \"C_condCode\", \"C_clientCode\"]]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Configurar o gráfico em 3D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plotar os dados, usando as três features e colorindo de acordo com os clusters\n",
    "scatter = ax.scatter(\n",
    "    df[\"measure_diff\"],\n",
    "    df[\"C_condCode\"],\n",
    "    df[\"C_clientCode\"],\n",
    "    c=df[\"cluster\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# Adicionar os centróides no gráfico\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    centroids[:, 2],\n",
    "    s=300,\n",
    "    c=\"red\",\n",
    "    label=\"Centróides\",\n",
    ")\n",
    "\n",
    "# Configurar rótulos e título\n",
    "ax.set_title(\"Clusters 3D - KMeans\")\n",
    "ax.set_xlabel(\"Measure_diff\")\n",
    "ax.set_ylabel(\"C_condCode\")\n",
    "ax.set_zlabel(\"C_clientCode\")\n",
    "\n",
    "# Adicionar uma legenda\n",
    "legend = ax.legend(loc=\"upper right\", title=\"Centróides\")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Definição do K e Justificativa\n",
    "\n",
    "Inicialmente, definimos o valor de **K = 3** de forma arbitrária para segmentar os consumidores. No entanto, para garantir uma escolha mais embasada, utilizamos duas métricas:\n",
    "\n",
    "1. **Elbow Method**: Essa métrica sugere o K ideal ao avaliar a variação na soma das distâncias dentro dos clusters. O gráfico do método Elbow indicou que o valor ideal de K estaria entre **4 e 5**, já que a curva começou a suavizar nesse ponto.\n",
    "\n",
    "2. **Silhouette Score**: Essa métrica mede a coesão e separação dos clusters. O Silhouette Score recomendou o valor de **K = 3**, indicando que, com 3 clusters, os grupos estavam mais bem definidos, com maior distância entre clusters e melhor compactação interna.\n",
    "\n",
    "Seguindo a recomendação do **Silhouette Score**, optamos por manter **K = 3**, pois essa métrica demonstrou que esse número de clusters proporcionava uma segmentação mais clara e precisa dos consumidores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do Elbow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = df[[\"measure_diff\", \"C_condCode\", \"C_clientCode\"]]\n",
    "\n",
    "features = features.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "k_values = range(1, 11)\n",
    "inertia = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, \"bo-\", markersize=8)\n",
    "plt.xlabel(\"Número de clusters (K)\")\n",
    "plt.ylabel(\"Inércia\")\n",
    "plt.title(\"Método Elbow para encontrar o melhor K\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico da Silhueta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Usando 10% dos dados como no exemplo anterior\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "X = df_sample[[\"measure_diff\", \"C_condCode\", \"C_clientCode\"]]\n",
    "X = X.dropna()\n",
    "\n",
    "# Defina o valor de K (número de clusters que você deseja usar)\n",
    "n_clusters = 3  # Exemplo, escolha o K apropriado\n",
    "\n",
    "# Criando o modelo KMeans e ajustando aos dados\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculando o coeficiente da silhueta para cada ponto\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(f\"Coeficiente médio de silhueta para K = {n_clusters}: {silhouette_avg}\")\n",
    "\n",
    "# Calcula os valores da silhueta para cada amostra\n",
    "sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "# Iniciando o gráfico\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "# Configuração do gráfico\n",
    "ax1.set_xlim([-0.1, 1])  # O coeficiente da silhueta vai de -1 a 1\n",
    "ax1.set_ylim(\n",
    "    [0, len(X) + (n_clusters + 1) * 10]\n",
    ")  # Para deixar espaço entre os clusters\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Agrega os valores da silhueta para o cluster i\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Rotula os clusters no meio do gráfico\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Calcula a nova posição do eixo y\n",
    "    y_lower = y_upper + 10  # 10 para o espaço entre os clusters\n",
    "\n",
    "# Adicionar títulos e labels ao gráfico\n",
    "ax1.set_title(\"Gráfico de Silhueta para os clusters\")\n",
    "ax1.set_xlabel(\"Coeficiente de Silhueta\")\n",
    "ax1.set_ylabel(\"Cluster\")\n",
    "\n",
    "# A linha vertical para a média do coeficiente de silhueta\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Remover labels y\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4. Escolha do Tipo de Sistema de Recomendação\n",
    "\n",
    "Optamos por um **sistema de recomendação baseado em análise de clusters e detecção de anomalias**, adequado para o cenário de monitoramento de consumo de gás. Essa abordagem permite que o sistema:\n",
    "\n",
    "- **Identifique padrões de consumo anômalos** que desviam do comportamento usual de um determinado cluster, o que pode indicar vazamentos ou falhas em equipamentos.\n",
    "- **Sugira áreas prioritárias para intervenção técnica**, utilizando dados de proximidade geográfica e consumo para identificar regiões que necessitam de manutenção preventiva.\n",
    "- **Compare o consumo individual com a média do grupo**, facilitando a detecção de discrepâncias que justifiquem inspeções ou ajustes, aumentando a eficiência no controle e gestão do consumo de gás.\n",
    "\n",
    "Essa escolha é justificada pela necessidade de um sistema que detecte variações no comportamento de consumo e priorize ações preventivas de forma precisa e ágil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abaixo segue análise de cliente que mais aparece com medidas acima de 20.\n",
    "\n",
    "Exemplo de como os insights do modelo devem ser utilizados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"measure_diff\"] > 20][\"C_clientCode\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF desse cliente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"C_clientCode\"] == 1685]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coluna datetime a partir da timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df[\"datetime\"] = df[\"timestamp\"].apply(\n",
    "    lambda x: (datetime.datetime.fromtimestamp(x) if pd.notnull(x) else None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuário vs média de seu condominio em diferentes escalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_clientCode\"] == 1685)][\"datetime\"],\n",
    "    df[(df[\"C_clientCode\"] == 1685)][\"measure_diff\"],\n",
    "    label=\"Consumo do outlier\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_condCode\"] == 36)][\"datetime\"].sort_values(ascending=True),\n",
    "    df[(df[\"C_condCode\"] == 36)][\"measure_avg_consumption\"],\n",
    "    label=\"Consumo médio do condomínio\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Consumo do outlier vs Consumo do condomínio\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuário vs Média de Consumo no Condomínio em Diferentes Escalas\n",
    "\n",
    "Neste gráfico, estamos comparando o consumo de um cliente específico (código 1685) com o consumo médio de um condomínio (código 36) ao longo do tempo. \n",
    "\n",
    "- **Pontos azuis**: Representam o consumo de um \"outlier\", ou seja, um cliente cujo consumo se distancia significativamente do comportamento esperado.\n",
    "- **Pontos laranja**: Representam o consumo médio de todos os usuários do condomínio 36 no mesmo período.\n",
    "\n",
    "Essa visualização é importante para identificarmos padrões anômalos em comparação com o consumo regular do condomínio. O comportamento do cliente fora da curva pode sugerir a necessidade de uma investigação mais detalhada, como possíveis fraudes, falhas no medidor ou vazamentos.\n",
    "\n",
    "A análise do gráfico mostra que o consumo do outlier tem um comportamento muito mais volátil e disperso do que a média estável do condomínio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_clientCode\"] == 1685) & (df[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df[(df[\"C_clientCode\"] == 1685) & (df[\"datetime\"] > \"2024-05\")][\"measure_diff\"],\n",
    "    label=\"Consumo do outlier\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_condCode\"] == 36) & (df[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df[(df[\"C_condCode\"] == 36) & (df[\"datetime\"] > \"2024-05\")][\n",
    "        \"measure_avg_consumption\"\n",
    "    ],\n",
    "    label=\"Consumo médio do condomínio\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Consumo do outlier vs Consumo do condomínio\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Refinada do Período após Maio de 2024\n",
    "\n",
    "Neste gráfico, restringimos a análise para datas posteriores a maio de 2024, focando em um intervalo mais recente de consumo. \n",
    "\n",
    "- **Escala Ajustada**: A escala do eixo Y foi ajustada para entre 0 e 1 para melhorar a visualização das diferenças no consumo médio e do outlier nesse período.\n",
    "- **Padrões Visíveis**: Embora o consumo médio continue estável, o consumo do outlier exibe picos que devem ser observados de perto.\n",
    "\n",
    "A escolha de um período mais curto nos permite visualizar com maior clareza qualquer variação anômala no comportamento de consumo do cliente, facilitando a identificação de eventos significativos para análises mais detalhadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_clientCode\"] == 1685) & (df[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df[(df[\"C_clientCode\"] == 1685) & (df[\"datetime\"] > \"2024-05\")][\"measure_diff\"],\n",
    "    label=\"Consumo do outlier\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df[(df[\"C_condCode\"] == 36) & (df[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df[(df[\"C_condCode\"] == 36) & (df[\"datetime\"] > \"2024-05\")][\n",
    "        \"measure_avg_consumption\"\n",
    "    ],\n",
    "    label=\"Consumo médio do condomínio\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Consumo do outlier vs Consumo do condomínio\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cálculo do consumo de cada consumidor, precisamos que o consumo leve em conta a ausência de acrescimo do pulseCount apesar do aumento do consumo. A ausência dessa relação incorre na falsa impressão de que um consumidor teve um alto consumo repentino, gerando spikes de consumo e dando uma falsa indicação\n",
    "Pegar o consumo e dividir entre o consumo médio acumulado entre medições\n",
    "\n",
    "Dividir o consumo médio pelo delta do tempo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 4: Competição entre modelos\n",
    "#### Abaixo desenvolveremos 3 modelos a partir de hipóteses e avaliações diferentes que serão explicados abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo do Consumo Médio e Detecção de Picos\n",
    "\n",
    "Nesta etapa, o objetivo é calcular o consumo de cada cliente levando em consideração a ausência de aumento no contador de pulsos (`pulseCount`), o que pode resultar em picos falsos de consumo. Para evitar essas inconsistências, o consumo médio entre medições é utilizado, dividido pelo delta de tempo entre cada leitura.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Ordenação dos Dados**: O dataframe é ordenado pelo código do cliente (`C_clientCode`) e pela data da medição (`datetime`), para garantir que as comparações entre leituras consecutivas façam sentido.\n",
    "2. **Criação da Coluna `match`**: A nova coluna `match` é criada para identificar se a linha atual pertence ao mesmo cliente que a linha anterior, facilitando a detecção de mudanças no cliente ou no medidor.\n",
    "\n",
    "Essa abordagem ajuda a evitar a detecção de falsos picos de consumo que podem ocorrer devido à falta de incremento do contador de pulsos, garantindo uma análise mais robusta dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.sort_values(by=[\"C_clientCode\", \"datetime\"], ascending=True)\n",
    "\n",
    "df_1[\"match\"] = df_1.clientCode.eq(df_1.clientCode.shift())\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.reset_index(drop=True, inplace=True)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_1[\"datetime\"][1] - df_1[\"datetime\"][0]\n",
    "print(x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "df_1[\"time_diff_secs\"] = None\n",
    "\n",
    "df_1[\"time_diff_secs\"] = df_1[\"match\"].where(df_1[\"match\"] == False, df_1[\"datetime\"].diff())\n",
    "\n",
    "df_1[\"time_diff_secs\"] = df_1[\"time_diff_secs\"].where(\n",
    "    df_1[\"match\"] != False,\n",
    "    timedelta(seconds=0).total_seconds(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo do Delta de Tempo Entre Medições\n",
    "\n",
    "Nesta etapa, estamos calculando a diferença de tempo (`time_diff_secs`) entre medições consecutivas para cada cliente, o que é fundamental para calcular o consumo corretamente ao longo do tempo.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Reset do Índice**: O índice do dataframe foi redefinido para garantir que as operações subsequentes sejam realizadas de forma sequencial após a ordenação anterior.\n",
    "2. **Cálculo da Diferença de Tempo**: A diferença de tempo é calculada usando a função `.diff()` para determinar o intervalo entre cada medição.\n",
    "   - Caso seja o mesmo cliente, o delta de tempo real é calculado.\n",
    "   - Caso o cliente mude entre as medições (valor `False` na coluna `match`), o delta de tempo é ajustado para `0` segundos, evitando cálculos incorretos.\n",
    "\n",
    "Este cálculo de delta de tempo é importante para garantir que as comparações de consumo sejam consistentes entre medições e que qualquer falha na continuidade da medição seja devidamente tratada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"time_diff_secs\"] = df_1[\"time_diff_secs\"].apply(\n",
    "    lambda x: x.total_seconds() if x != 0.0 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1[\"C_clientCode\"] == 1716.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"measure_by_time\"] = df_1[\"measure_diff\"] / df_1[\"time_diff_secs\"]\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste do Delta de Tempo e Cálculo do Consumo por Unidade de Tempo\n",
    "\n",
    "Nesta etapa, estamos refinando o cálculo do delta de tempo e utilizando essa métrica para calcular o consumo por unidade de tempo para cada cliente.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Ajuste do Delta de Tempo**: O delta de tempo em segundos foi refinado utilizando a função `apply()` com uma `lambda function`. Isso garante que os casos onde o delta de tempo é zero sejam tratados adequadamente. Se o delta for zero, mantemos o valor de zero, caso contrário, convertemos para o total em segundos.\n",
    "2. **Filtragem por Cliente**: Filtramos os dados para focar no cliente de código `1716.0`, permitindo uma análise mais detalhada de um único cliente para verificar seus padrões de consumo.\n",
    "3. **Cálculo do Consumo por Tempo (`measure_by_time`)**: O consumo diferenciado (`measure_diff`) é dividido pelo delta de tempo em segundos para obter o consumo por unidade de tempo. Essa métrica é essencial para padronizar o consumo e identificar padrões de comportamento ao longo do tempo.\n",
    "\n",
    "Essas operações ajudam a garantir que o consumo seja calculado de forma precisa, mesmo em intervalos de tempo irregulares, o que facilita a detecção de anomalias no comportamento de consumo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"measure_by_time\"] = df_1[\"measure_by_time\"].replace(np.inf, 0)\n",
    "df_1[\"measure_by_time\"] = df_1[\"measure_by_time\"].replace(np.nan, 0)\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalização dos Dados e Clustering com K-Means\n",
    "\n",
    "Nesta etapa, estamos preparando os dados para a aplicação do algoritmo de clustering K-Means. O objetivo é identificar grupos (clusters) de comportamento de consumo similares entre os clientes.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Tratamento de Valores Infinitos e Nulos**: \n",
    "   - Primeiramente, os valores infinitos e `NaN` na coluna `measure_by_time` são substituídos por zero, garantindo que o dataset não contenha valores inválidos que possam comprometer o algoritmo.\n",
    "   - Em seguida, removemos qualquer linha que contenha valores nulos nas colunas de features selecionadas.\n",
    "\n",
    "2. **Normalização dos Dados**: \n",
    "   - Usamos o `StandardScaler` para normalizar os dados, o que é crucial para algoritmos como o K-Means, já que ele é sensível à escala das features.\n",
    "\n",
    "3. **Aplicação do K-Means**: \n",
    "   - Testamos diferentes valores de `K` (número de clusters) para o K-Means, variando de 1 a 10 clusters.\n",
    "   - Para cada valor de `K`, calculamos a inércia, que é a soma das distâncias quadradas dentro de cada cluster.\n",
    "\n",
    "4. **Plot do Método Elbow**: \n",
    "   - O gráfico \"Elbow\" (cotovelo) é gerado para visualizar a inércia em função do número de clusters. O ponto onde o gráfico faz uma \"curva\" é sugerido como o valor ideal de `K`.\n",
    "   - Isso ajuda a determinar o número de clusters que melhor segmenta os padrões de consumo dos clientes.\n",
    "\n",
    "Essa análise nos permite agrupar clientes com padrões de consumo similares, facilitando a identificação de anomalias ou comportamentos fora do padrão em cada cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = df_1[[\"measure_by_time\", \"C_condCode\", \"C_clientCode\"]]\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "features = features.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "k_values = range(1, 11)\n",
    "inertia = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, \"bo-\", markersize=8)\n",
    "plt.xlabel(\"Número de clusters (K)\")\n",
    "plt.ylabel(\"Inércia\")\n",
    "plt.title(\"Método Elbow para encontrar o melhor K\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinamento do Consumo por Tempo e Análise por Data\n",
    "\n",
    "**Gráfico Elbow**: O gráfico acima mostra o resultado da aplicação do **método Elbow** para identificar o número ideal de clusters. Aqui, podemos observar que a inércia diminui drasticamente até cerca de 3-4 clusters, o que sugere que esse pode ser o número ideal de clusters para segmentar os dados.\n",
    "\n",
    "Após essa análise, continuamos refinando o dataset:\n",
    "\n",
    "**Etapas:**\n",
    "1. **Tratamento de Valores Infinitos e Nulos**: Assim como nas etapas anteriores, os valores infinitos (`np.inf`) e `NaN` são removidos para garantir a integridade dos dados.\n",
    "   \n",
    "2. **Escalonamento do Consumo**: \n",
    "   - A coluna `measure_by_time` é escalada por um fator de 100, armazenada na nova coluna `measure_by_time_scaled`, para normalizar os valores de consumo e facilitar comparações subsequentes.\n",
    "   - Filtramos para manter apenas os registros onde o consumo escalado é maior que zero, garantindo que apenas dados válidos sejam considerados.\n",
    "\n",
    "3. **Análise por Data e Condomínio**: \n",
    "   - Normalizamos a data (`datetime`) para criar uma coluna no formato `YYYY-MM-DD`, facilitando a agregação por dias.\n",
    "   - Para cada data única e cada condomínio, calculamos o consumo médio diário (`measure_by_time_avg_consumption`), garantindo que tenhamos uma métrica confiável de consumo para cada dia e condomínio.\n",
    "\n",
    "Este refinamento é essencial para garantir a consistência dos dados ao longo do tempo e facilitar a análise de padrões diários de consumo, permitindo que anomalias possam ser detectadas em diferentes escalas temporais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"measure_by_time\"] = df_1[\"measure_by_time\"].replace([np.inf, -np.inf], np.nan)\n",
    "df_1[\"measure_by_time\"] = df_1[\"measure_by_time\"].dropna()\n",
    "\n",
    "df_1[\"measure_by_time_scaled\"] = df_1[\"measure_by_time\"] * 100\n",
    "df_1 = df_1[df_1[\"measure_by_time_scaled\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.reset_index(inplace=True, drop=True)\n",
    "df_1[\"datetime_yyyy-mm-dd\"] = df_1[\"datetime\"].dt.normalize()\n",
    "\n",
    "for date in np.array(df_1[\"datetime_yyyy-mm-dd\"].unique()):\n",
    "    measure_days_df_1 = df_1[df_1[\"datetime_yyyy-mm-dd\"] == date]\n",
    "    for cond in np.array(measure_days_df_1[\"C_condCode\"].unique()):\n",
    "        measure_avg_consumption = measure_days_df_1[\n",
    "            measure_days_df_1[\"C_condCode\"] == cond\n",
    "        ][\"measure_by_time\"].mean()\n",
    "        df_1.loc[\n",
    "            ((df_1[\"datetime_yyyy-mm-dd\"] == date) & (df_1[\"C_condCode\"] == cond)),\n",
    "            \"measure_by_time_avg_consumption\",\n",
    "        ] = measure_avg_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação do Consumo Total vs Consumo Médio ao Longo do Tempo\n",
    "\n",
    "Neste gráfico, estamos comparando duas métricas importantes de consumo ao longo do tempo:\n",
    "\n",
    "1. **Consumo Total (Medido)**:\n",
    "   - Representado pelos **pontos azuis**, essa métrica (`measure_by_time`) indica o consumo total medido para cada ponto no tempo.\n",
    "   \n",
    "2. **Consumo Médio**:\n",
    "   - Representado pelos **pontos laranja**, essa métrica (`measure_by_time_avg_consumption`) reflete o consumo médio diário do condomínio. É útil para observar a tendência geral de consumo ao longo do tempo.\n",
    "\n",
    "**Objetivo do Gráfico**:\n",
    "- Visualizar como o consumo total se comporta em relação à média.\n",
    "- Identificar períodos onde o consumo total se desvia da média, o que pode indicar anomalias, picos de consumo ou eventos específicos que devem ser investigados.\n",
    "\n",
    "**Configurações do Gráfico**:\n",
    "- O eixo Y está limitado entre 0 e 0.01 para facilitar a visualização de pequenas variações de consumo ao longo do tempo.\n",
    "- A rotação dos rótulos de data no eixo X foi ajustada para melhorar a legibilidade das datas ao longo do tempo.\n",
    "\n",
    "Esse gráfico é útil para identificar possíveis outliers, picos de consumo e avaliar a eficiência energética ao comparar o comportamento real com a média.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[\"datetime\"],\n",
    "    df_1[\"measure_by_time\"],\n",
    "    label=\"Measure Consumption\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[\"datetime\"],\n",
    "    df_1[\"measure_by_time_avg_consumption\"],\n",
    "    label=\"Average Consumption\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Total vs Average Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Selecionar as features para o KMeans\n",
    "X = df_1[[\"measure_by_time\", \"C_condCode\", \"C_clientCode\"]]\n",
    "\n",
    "# Aplicar o KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df_1[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Configurar o gráfico em 3D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plotar os dados, usando as três features e colorindo de acordo com os clusters\n",
    "scatter = ax.scatter(\n",
    "    df_1[\"measure_by_time\"],\n",
    "    df_1[\"C_condCode\"],\n",
    "    df_1[\"C_clientCode\"],\n",
    "    c=df_1[\"cluster\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    ")\n",
    "\n",
    "# Adicionar os centróides no gráfico\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    centroids[:, 2],\n",
    "    s=300,\n",
    "    c=\"red\",\n",
    "    label=\"Centróides\",\n",
    ")\n",
    "\n",
    "# Configurar rótulos e título\n",
    "ax.set_title(\"Clusters 3D - KMeans\")\n",
    "ax.set_xlabel(\"Measure_by_time\")\n",
    "ax.set_ylabel(\"C_condCode\")\n",
    "ax.set_zlabel(\"C_clientCode\")\n",
    "\n",
    "# Adicionar uma legenda\n",
    "legend = ax.legend(loc=\"upper right\", title=\"Centróides\")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização 3D dos Clusters com K-Means\n",
    "\n",
    "Neste gráfico 3D, utilizamos o algoritmo K-Means para agrupar os dados de consumo com base em três variáveis principais:\n",
    "- **Measure_by_time**: Consumo por unidade de tempo.\n",
    "- **C_condCode**: Código do condomínio.\n",
    "- **C_clientCode**: Código do cliente.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Aplicação do K-Means**: \n",
    "   - O algoritmo foi ajustado para gerar 3 clusters com base nas features selecionadas.\n",
    "   - O resultado da previsão (`cluster`) é adicionado ao dataframe para colorir os pontos no gráfico de acordo com os clusters atribuídos.\n",
    "\n",
    "2. **Visualização em 3D**: \n",
    "   - As três dimensões do gráfico representam as variáveis de entrada. Cada ponto é colorido conforme o cluster ao qual pertence, permitindo uma visão clara da segmentação dos dados.\n",
    "   - Foram adicionados os **centroides** dos clusters em vermelho, que representam o \"centro\" de cada grupo. Esses pontos ajudam a visualizar onde estão concentradas as características médias de cada cluster.\n",
    "\n",
    "**Objetivo do Gráfico**:\n",
    "- A visualização 3D permite explorar a distribuição dos dados e identificar como os clientes e condomínios estão agrupados com base no consumo. \n",
    "- Clusters bem definidos podem ajudar a identificar padrões de consumo similares e distinguir outliers ou comportamentos incomuns.\n",
    "\n",
    "Este tipo de análise é útil para segmentar clientes com comportamentos similares e entender padrões de consumo em diferentes condomínios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_outlier = df_1[df_1[\"measure_by_time\"] > 0.01][\"C_clientCode\"].value_counts().index[0]\n",
    "target_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[df_1[\"C_clientCode\"] == target_outlier][\"datetime_yyyy-mm-dd\"],\n",
    "    df_1[df_1[\"C_clientCode\"] == target_outlier][\"measure_by_time\"],\n",
    "    label=\"Measure Consumption\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    df_1[df_1[\"C_clientCode\"] == target_outlier][\"datetime_yyyy-mm-dd\"],\n",
    "    df_1[df_1[\"C_clientCode\"] == target_outlier][\"measure_by_time_avg_consumption\"],\n",
    "    label=\"Average Consumption\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Total vs Average Consumption Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "# plt.ylim(0, 0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificação e Análise de Outlier no Consumo\n",
    "\n",
    "Nesta etapa, estamos focando em identificar um **outlier** no consumo, ou seja, um cliente cujo consumo é significativamente maior que o padrão observado.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Identificação do Outlier**:\n",
    "   - Usamos um filtro para identificar o cliente cujo `measure_by_time` (consumo por tempo) excede o limite de 0.01, sendo considerado um consumo fora do normal.\n",
    "   - A variável `target_outlier` armazena o código do cliente identificado como outlier, para que possamos focar em seu comportamento de consumo.\n",
    "\n",
    "2. **Comparação do Consumo Total com a Média**:\n",
    "   - No gráfico, o **consumo total** do outlier é representado pelos **pontos azuis**, enquanto o **consumo médio** é mostrado pelos **pontos laranja**.\n",
    "   - Esta visualização permite observar claramente a discrepância entre o consumo do outlier e a média do condomínio ao longo do tempo.\n",
    "\n",
    "**Objetivo do Gráfico**:\n",
    "- Monitorar o comportamento do cliente fora do padrão e verificar se há picos significativos em determinados períodos.\n",
    "- Comparar o consumo total com a média para entender o quanto esse cliente se distancia do padrão de consumo geral.\n",
    "\n",
    "Este tipo de análise é crucial para identificar anomalias de consumo que podem indicar problemas como vazamentos, fraudes ou uso atípico de energia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1[\"C_clientCode\"] == target_outlier].sort_values(by=\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1[\"C_clientCode\"] == target_outlier][\"datetime_yyyy-mm-dd\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordenação e Análise Temporal do Outlier\n",
    "\n",
    "Nesta etapa, estamos examinando mais de perto o comportamento do cliente identificado como outlier. O objetivo é entender melhor o padrão de consumo desse cliente ao longo do tempo.\n",
    "\n",
    "**Etapas:**\n",
    "1. **Ordenação por Data**:\n",
    "   - O dataframe foi filtrado para o `target_outlier` (cliente fora do padrão), e os dados foram ordenados pela coluna `datetime` para garantir que as análises sigam a sequência cronológica correta.\n",
    "   - Isso nos permite observar como o consumo evolui ao longo do tempo e se há padrões temporais consistentes de consumo elevado.\n",
    "\n",
    "2. **Identificação de Datas Únicas**:\n",
    "   - Usamos o método `.unique()` na coluna de datas para listar todas as datas únicas em que há registros de consumo do outlier.\n",
    "   - Isso facilita a análise dos dias em que o consumo foi registrado, permitindo identificar se o comportamento anômalo se concentra em datas específicas ou é distribuído de forma mais uniforme.\n",
    "\n",
    "**Objetivo**:\n",
    "- Através dessa análise detalhada, conseguimos observar o comportamento do outlier ao longo de diferentes períodos, o que pode ajudar a identificar padrões sazonais, eventos específicos ou falhas no sistema de medição.\n",
    "- Essa investigação mais profunda pode ser usada para alertar os operadores de possíveis problemas ou anomalias no consumo de energia desse cliente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa abordagem busca melhorar a visualização dos consumos negativos, que foram muito proeminentes nas análises anteriores, expandindo o que apresentamos na última sprint. Para isso, segmentamos a base de dados em duas partes: indivíduos que apresentam consumo negativo e aqueles que não apresentam. Dentro de cada grupo, aplicamos técnicas de clusterização para identificar padrões que possam explicar os consumos negativos.\n",
    "\n",
    "### Etapas da Análise\n",
    "\n",
    "1. **Segmentação da Base de Dados:** A base foi dividida em dois subconjuntos: \n",
    "   - Pessoas com consumo negativo.\n",
    "   - Pessoas sem consumo negativo.\n",
    "\n",
    "2. **Clusterização Inicial com K-Means:**\n",
    "   - Em cada subconjunto, aplicamos o algoritmo K-Means para identificar agrupamentos dentro das duas populações.\n",
    "   - Utilizamos o método do cotovelo (Elbow Method) para determinar o número ideal de clusters para cada grupo.\n",
    "   - Inicialmente, o K-Means não gerou agrupamentos suficientemente distintos para uma análise aprofundada.\n",
    "\n",
    "3. **Ajuste do Número de Clusters:**\n",
    "   - Diante dos resultados iniciais, aumentamos o número de clusters para verificar se uma divisão mais granular melhoraria a separação dos grupos.\n",
    "   - Essa abordagem trouxe uma leve melhora na identificação de padrões, mas ainda não foi o suficiente para explicar completamente os consumos negativos.\n",
    "\n",
    "4. **Identificação de Anomalias com Isolation Forest:**\n",
    "   - Para complementar a análise, aplicamos o algoritmo Isolation Forest, que é especializado na detecção de anomalias.\n",
    "   - Utilizamos o Grid Search para otimizar os hiperparâmetros do Isolation Forest, buscando o melhor ajuste para identificar comportamentos atípicos dentro dos clusters.\n",
    "\n",
    "### Resultados e Próximos Passos\n",
    "\n",
    "- A combinação de K-Means e Isolation Forest permitiu uma identificação mais refinada de padrões e anomalias entre os indivíduos com consumo negativo.\n",
    "- A análise revelou que, em alguns casos, os consumos negativos podem ser atribuídos a comportamentos específicos que se destacam como outliers nos clusters.\n",
    "- Os próximos passos incluem a análise mais detalhada dos clusters e das anomalias identificadas, com o objetivo de compreender as causas subjacentes dos consumos negativos e propor intervenções específicas.\n",
    "\n",
    "Com essa abordagem, buscamos não apenas mapear os padrões de consumo, mas também entender as anomalias que surgem nos dados, oferecendo uma visão mais completa e direcionada para a resolução desses casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sort_values(by=[\"C_clientCode\", \"datetime\"], ascending=True)\n",
    "\n",
    "df2[\"match\"] = df2.clientCode.eq(df2.clientCode.shift())\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(drop=True, inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2[\"datetime\"][1] - df2[\"datetime\"][0]\n",
    "print(x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "df2[\"time_diff_secs\"] = None\n",
    "\n",
    "df2[\"time_diff_secs\"] = df2[\"match\"].where(df2[\"match\"] == False, df2[\"datetime\"].diff())\n",
    "\n",
    "df2[\"time_diff_secs\"] = df2[\"time_diff_secs\"].where(\n",
    "    df2[\"match\"] != False, timedelta(seconds=0).total_seconds(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"time_diff_secs\"] = df2[\"time_diff_secs\"].apply(\n",
    "    lambda x: x.total_seconds() if x != 0.0 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando por cliente e verificando se existe ao menos um valor negativo em 'measure_avg_consumption'\n",
    "df2['tem_negativos'] = df2.groupby('C_clientCode')['measure_avg_consumption'].transform(lambda x: (x < 0).any())\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Filtrando os dados para clientes com medições negativas\n",
    "df_negativos = df2[df2['tem_negativos'] == True][['C_condCode', 'measure_avg_consumption']]\n",
    "df_positivos = df2[df2['tem_negativos'] == False][['C_condCode', 'measure_avg_consumption']]\n",
    "\n",
    "# Número de clusters definidos a partir dos resultados do Elbow e Silhueta\n",
    "n_clusters_negativos = 7  # Defina o número de clusters ideal para clientes com medições negativas\n",
    "n_clusters_positivos = 3  # Defina o número de clusters ideal para clientes sem medições negativas\n",
    "\n",
    "# Aplicando o K-Means para clientes com medições negativas\n",
    "kmeans_negativos = KMeans(n_clusters=n_clusters_negativos, random_state=42)\n",
    "df_negativos['cluster'] = kmeans_negativos.fit_predict(df_negativos)\n",
    "\n",
    "# Aplicando o K-Means para clientes sem medições negativas\n",
    "kmeans_positivos = KMeans(n_clusters=n_clusters_positivos, random_state=42)\n",
    "df_positivos['cluster'] = kmeans_positivos.fit_predict(df_positivos)\n",
    "\n",
    "# Plotando os clusters para clientes com medições negativas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='C_condCode', y='measure_avg_consumption', hue='cluster', data=df_negativos, palette='Set1')\n",
    "plt.title('Clientes com Medições Negativas')\n",
    "plt.xlabel('C_condCode')\n",
    "plt.ylabel('Medição Média de Consumo')\n",
    "\n",
    "# Plotando os clusters para clientes sem medições negativas\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='C_condCode', y='measure_avg_consumption', hue='cluster', data=df_positivos, palette='Set2')\n",
    "plt.title('Clientes sem Medições Negativas')\n",
    "plt.xlabel('C_condCode')\n",
    "plt.ylabel('Medição Média de Consumo')\n",
    "\n",
    "# Exibindo os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exibindo os resultados dos clusters\n",
    "print(\"Clusters para clientes com medições negativas:\")\n",
    "print(df_negativos)\n",
    "\n",
    "print(\"\\nClusters para clientes sem medições negativas:\")\n",
    "print(df_positivos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filtrando os dados para clientes com medições negativas\n",
    "df_negativos = df2[df2['tem_negativos'] == True][['C_condCode', 'measure_avg_consumption']]\n",
    "df_positivos = df2[df2['tem_negativos'] == False][['C_condCode', 'measure_avg_consumption']]\n",
    "\n",
    "# Aplicando o Isolation Forest para clientes com medições negativas\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Define 5% dos dados como outliers\n",
    "df_negativos['anomaly'] = iso_forest.fit_predict(df_negativos)\n",
    "\n",
    "# O Isolation Forest retorna -1 para anomalias (outliers) e 1 para pontos normais\n",
    "df_negativos['anomaly'] = df_negativos['anomaly'].apply(lambda x: 'Outlier' if x == -1 else 'Normal')\n",
    "\n",
    "# Plotando os resultados do Isolation Forest para clientes com medições negativas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='C_condCode', y='measure_avg_consumption', hue='anomaly', data=df_negativos, palette='Set1')\n",
    "plt.title('Clientes com Medições Negativas (Anomalias)')\n",
    "plt.xlabel('C_condCode')\n",
    "plt.ylabel('Medição Média de Consumo')\n",
    "\n",
    "# Aplicando o K-Means para clientes sem medições negativas (mantendo como K-Means para comparação)\n",
    "n_clusters_positivos = 3  # Defina o número de clusters ideal para clientes sem medições negativas\n",
    "kmeans_positivos = KMeans(n_clusters=n_clusters_positivos, random_state=42)\n",
    "df_positivos['cluster'] = kmeans_positivos.fit_predict(df_positivos)\n",
    "\n",
    "# Plotando os clusters para clientes sem medições negativas\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='C_condCode', y='measure_avg_consumption', hue='cluster', data=df_positivos, palette='Set2')\n",
    "plt.title('Clientes sem Medições Negativas')\n",
    "plt.xlabel('C_condCode')\n",
    "plt.ylabel('Medição Média de Consumo')\n",
    "\n",
    "# Exibindo os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exibindo os resultados do Isolation Forest\n",
    "print(\"Anomalias para clientes com medições negativas:\")\n",
    "print(df_negativos)\n",
    "\n",
    "print(\"\\nClusters para clientes sem medições negativas:\")\n",
    "print(df_positivos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Filtrando os dados para clientes com medições negativas\n",
    "df_negativos = df2[df2['tem_negativos'] == True][['C_condCode', 'measure_avg_consumption']]\n",
    "\n",
    "# Definindo o modelo de Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Definindo o Grid de Hiperparâmetros para testar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Testar 50, 100 e 200 árvores\n",
    "    'max_samples': ['auto', 0.6],  # Auto, 60%, 80% ou 100% das amostras\n",
    "    'contamination': [0.01, 0.05, 0.1],  # Testar 1%, 5% e 10% de contaminação (outliers)\n",
    "    'max_features': [1.0, 0.5, 0.75]  # Testar todas as features, 50%, 75% das features\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=iso_forest,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Métrica de avaliação\n",
    "    cv=5,  # Validação cruzada com 5 folds\n",
    "    verbose=1,  # Exibir progresso\n",
    "    n_jobs=-1  # Usar todos os processadores disponíveis\n",
    ")\n",
    "\n",
    "# Rodando o GridSearch\n",
    "grid_search.fit(df_negativos)\n",
    "\n",
    "# Exibindo os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros encontrados pelo GridSearch:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Exibindo o melhor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Melhor modelo de Isolation Forest:\")\n",
    "print(best_model)\n",
    "\n",
    "# Aplicando o melhor modelo aos dados\n",
    "df_negativos['anomaly'] = best_model.predict(df_negativos)\n",
    "\n",
    "# Convertendo -1 para \"Outlier\" e 1 para \"Normal\"\n",
    "df_negativos['anomaly'] = df_negativos['anomaly'].apply(lambda x: 'Outlier' if x == -1 else 'Normal')\n",
    "\n",
    "# Plotando os resultados\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='C_condCode', y='measure_avg_consumption', hue='anomaly', data=df_negativos, palette='Set1')\n",
    "plt.title('Clientes com Medições Negativas (Anomalias com Isolation Forest)')\n",
    "plt.xlabel('C_condCode')\n",
    "plt.ylabel('Medição Média de Consumo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Função para calcular e plotar o método Elbow\n",
    "def plot_elbow(df2, cluster_range, title):\n",
    "    inertia = []\n",
    "    \n",
    "    for k in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(df2)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot do método Elbow\n",
    "    plt.plot(cluster_range, inertia, marker='o')\n",
    "    plt.title(f'Método Elbow - {title}')\n",
    "    plt.xlabel('Número de Clusters')\n",
    "    plt.ylabel('Inércia (Soma dos Erros Quadráticos)')\n",
    "    plt.show()\n",
    "\n",
    "cluster_range = range(1, 10)\n",
    "# Clientes sem medições negativas\n",
    "print(\"\\nClientes sem medições negativas:\")\n",
    "plot_elbow(df_positivos, cluster_range, 'Clientes sem Medições Negativas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como no primeiro modelo, nos motivamos a comparar um indivíduo contra a média de seu condomínio, mas dessa vez em dias específicos.\n",
    "### 1. **Preprocessamento dos Dados**\n",
    "   - **Conversão da Coluna de Tempo**: A coluna `timestamp` foi convertida para um formato de data e hora (`datetime`) para facilitar a manipulação dos dados.\n",
    "   - **Filtro por Dia da Semana**: O código foi ajustado para filtrar os dados de consumo de acordo com o dia da semana escolhido.\n",
    "   - **Ordenação dos Dados**: Os dados foram ordenados por condomínio (`condCode`), cliente (`clientCode`) e data (`datetime`) para garantir a consistência temporal das medições.\n",
    "   - **Cálculo da Diferença de Tempo**: A diferença de tempo entre medições consecutivas foi calculada (`time_diff`), e os valores negativos ou nulos foram removidos para evitar problemas com cálculos de consumo.\n",
    "### 2. **Tratamento de Valores de Consumo**\n",
    "   - **Cálculo da Diferença de Consumo**: A coluna `measure_diff` foi criada para calcular a diferença de consumo entre medições consecutivas para cada cliente.\n",
    "   - **Remoção de Valores Negativos**: Como valores negativos de consumo não fazem sentido no contexto do projeto, eles foram removidos do dataset.\n",
    "   - **Cálculo do Consumo por Segundo**: A taxa de consumo por segundo foi calculada dividindo `measure_diff` por `time_diff`, criando a coluna `consumption_seconds`. Essa abordagem normaliza o consumo considerando a irregularidade nas medições temporais.\n",
    "### 3. **Agrupamento de Consumo**\n",
    "   - **Consumo Individual**: Para cada cliente, foi calculado o consumo total normalizado ao longo do dia, somando a coluna `consumption_seconds`. O resultado foi armazenado na coluna `total_individual_consumption`.\n",
    "   - **Média de Consumo do Condomínio**: A média de consumo de todos os clientes dentro de cada condomínio, em um determinado dia, foi calculada e armazenada na coluna `mean_cond`.\n",
    "### 4. **Método do Cotovelo (Elbow Method)**\n",
    "   - O **método do cotovelo** foi aplicado para determinar o número ideal de clusters ao aplicar o algoritmo **K-Means**. A inércia (soma dos quadrados das distâncias dos pontos ao centróide mais próximo) foi calculada para diferentes valores de K (número de clusters).\n",
    "   - Um gráfico foi gerado para identificar o ponto de \"cotovelo\", onde o ganho em separar os dados em mais clusters começa a diminuir, indicando o número ideal de clusters.\n",
    "### 5. **Aplicação de K-Means**\n",
    "   - Após determinar o número ideal de clusters pelo método do cotovelo, o algoritmo **K-Means** foi aplicado para agrupar os clientes com base no consumo total individual (`total_individual_consumption`) e na média de consumo do condomínio (`mean_cond`).\n",
    "   - Os resultados da clusterização foram armazenados na coluna `cluster`, representando a qual grupo cada cliente pertence.\n",
    "### 6. **Detecção de Anomalias com DBSCAN**\n",
    "   - O algoritmo **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) foi utilizado para detectar anomalias no comportamento de consumo dos clientes.\n",
    "   - Dois parâmetros principais foram ajustados:\n",
    "     - **`eps` (raio de alcance)**: Determina a distância máxima para que dois pontos sejam considerados vizinhos.\n",
    "     - **`min_samples`**: Define o número mínimo de clientes próximos necessários para formar um cluster.\n",
    "   - Clientes cujos padrões de consumo estavam muito distantes dos demais foram classificados como **anomalias** e marcados com o rótulo `-1` pelo DBSCAN.\n",
    "   - Um gráfico de **K-vizinho mais próximo (KNN Distance Plot)** foi utilizado para ajustar o valor de `eps`. O ponto onde a curva da distância começa a subir rapidamente foi identificado como o valor ideal para `eps`.\n",
    "### 7. **Visualizações e Interpretação**\n",
    "   - Várias visualizações foram geradas para facilitar a interpretação dos resultados, incluindo:\n",
    "     - **Gráfico K-Vizinho Mais Próximo**: Utilizado para ajustar o parâmetro `eps` no DBSCAN.\n",
    "     - **Gráfico do Método do Cotovelo**: Utilizado para determinar o número ideal de clusters para o K-Means.\n",
    "     - **Gráficos de Dispersão (Scatter Plots)**: Visualizaram os clientes em relação ao consumo individual e ao consumo médio do condomínio, destacando as anomalias detectadas pelo DBSCAN.\n",
    "### 8. **Análise Final e Conclusões**\n",
    "   - A análise dos clusters ajudou a identificar grupos de clientes com comportamentos de consumo semelhantes, facilitando a interpretação dos padrões de consumo de energia.\n",
    "   - O **DBSCAN** foi especialmente útil na detecção de **anomalias**, ou seja, clientes que apresentaram um comportamento de consumo muito diferente do esperado. Esses clientes foram marcados como \"anomalias\" e podem representar fraudes, erros de medição ou mudanças inesperadas no padrão de consumo.\n",
    "   - A utilização de técnicas de clusterização e detecção de anomalias forneceu insights valiosos sobre o comportamento de consumo dos clientes, permitindo identificar padrões e possíveis problemas de forma eficiente.\n",
    "Esse resumo cobre todas as alterações e melhorias feitas no projeto. Ele documenta de forma clara o que foi implementado e como as diferentes técnicas foram utilizadas para alcançar os resultados desejados. Caso precise de mais ajustes ou explicações detalhadas sobre algum ponto específico, estou à disposição!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.sort_values(by=[\"C_clientCode\", \"datetime\"], ascending=True)\n",
    "\n",
    "df3[\"match\"] = df3.clientCode.eq(df3.clientCode.shift())\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index(drop=True, inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df3[\"datetime\"][1] - df3[\"datetime\"][0]\n",
    "print(x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "df3[\"time_diff_secs\"] = None\n",
    "\n",
    "df3[\"time_diff_secs\"] = df3[\"match\"].where(df3[\"match\"] == False, df3[\"datetime\"].diff())\n",
    "\n",
    "df3[\"time_diff_secs\"] = df3[\"time_diff_secs\"].where(\n",
    "    df3[\"match\"] != False, timedelta(seconds=0).total_seconds(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"time_diff_secs\"] = df3[\"time_diff_secs\"].apply(\n",
    "    lambda x: x.total_seconds() if x != 0.0 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cria uma coluna com o nome do dia da semana\n",
    "df3['day_name'] = df3['datetime'].dt.day_name()\n",
    "\n",
    "# Filtrar para o dia escolhido\n",
    "df3 = df3[df3['day_name'] == 'Sunday']\n",
    "\n",
    "# Garantir que os dados estão ordenados corretamente por condomínio (condCode), cliente (clientCode) e data\n",
    "df3 = df3.sort_values(by=['condCode', 'clientCode', 'datetime'])\n",
    "\n",
    "df3['time_diff'] = df3.groupby('clientCode')['datetime'].diff().dt.total_seconds()\n",
    "\n",
    "# df3 = df3.dropna(subset=['measure_diff', 'time_diff'])\n",
    "\n",
    "# Evitar divisões por zero ou valores negativos de tempo\n",
    "df3 = df3[df3['time_diff'] > 0]\n",
    "\n",
    "# Calcular a taxa de consumo (consumo por segundo) entre as medições\n",
    "df3['consumption_seconds'] = df3['measure_diff'] / df3['time_diff']\n",
    "\n",
    "# Agrupar por condCode, clientCode e dia da semana e calcular a diferença de consumo para cada grupo\n",
    "individual_result = df3.groupby(['condCode', 'clientCode', 'day_name'])['consumption_seconds'].sum().reset_index(name='total_individual_consumption')\n",
    "\n",
    "cond_result = df3.groupby(['condCode', 'day_name'])['consumption_seconds'].mean().reset_index(name='mean_cond')\n",
    "\n",
    "df_result = individual_result.merge(cond_result, on=['condCode', 'day_name'], how='left')\n",
    "\n",
    "print(df_result.head())\n",
    "\n",
    "X = df_result[['total_individual_consumption', 'mean_cond']]\n",
    "\n",
    "# X = X.dropna()\n",
    "\n",
    "# Escalar os dados para a clusterização\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar o método do cotovelo para encontrar o número ideal de clusters\n",
    "inertia = []  # Lista para armazenar as inércias\n",
    "K = range(1, 11)  # Testando de 1 até 10 clusters\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotar o gráfico do método do cotovelo\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K, inertia, 'bo-', markersize=8)\n",
    "plt.xlabel('Número de Clusters (K)', fontsize=14)\n",
    "plt.ylabel('Inércia', fontsize=14)\n",
    "plt.title('Método do Cotovelo para Encontrar o Número Ideal de Clusters', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means com o número de clusters ideal após analisar o gráfico do método do cotovelo\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)  # Substitua 3 pelo número ideal encontrado\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Adicionar os rótulos de cluster ao DataFrame original\n",
    "df_result['cluster'] = kmeans.labels_\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Reverter a escala dos centros para o espaço original\n",
    "cluster_centers_og = scaler.inverse_transform(cluster_centers)\n",
    "\n",
    "# Exibir as primeiras linhas para verificar o resultado dos clusters\n",
    "print(df_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1 = df_result[df_result['cluster'] == 1]\n",
    "\n",
    "print(cluster_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Configurar DBSCAN\n",
    "dbscan = DBSCAN(eps=0.016, min_samples=3)  # eps define o raio de alcance para formar um cluster, min_samples define o número mínimo de pontos para formar um cluster\n",
    "# Aplicar DBSCAN nos dados de consumo\n",
    "X_dbscan = df_result[['total_individual_consumption', 'mean_cond']]\n",
    "df_result['dbscan_cluster'] = dbscan.fit_predict(X_dbscan)\n",
    "# DBSCAN atribui o valor -1 para anomalias\n",
    "df_result['dbscan_anomalia'] = df_result['dbscan_cluster'].apply(lambda x: 'Anomaly' if x == -1 else 'Normal')\n",
    "# Visualizar os resultados\n",
    "print(\"Anomalias detectadas pelo DBSCAN:\")\n",
    "print(df_result[df_result['dbscan_anomalia'] == 'Anomaly'])\n",
    "# Visualizar as anomalias detectadas pelo DBSCAN\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='total_individual_consumption', y='mean_cond', hue='dbscan_anomalia', data=df_result, palette='Set1', s=100)\n",
    "plt.title('Anomalias Detectadas pelo DBSCAN')\n",
    "plt.xlabel('Consumo Total Individual')\n",
    "plt.ylabel('Média do Consumo do Condomínio')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "# Definir o número de vizinhos (min_samples)\n",
    "min_samples = 5\n",
    "# Ajustar o modelo Nearest Neighbors\n",
    "neigh = NearestNeighbors(n_neighbors=min_samples)\n",
    "nbrs = neigh.fit(X_dbscan)\n",
    "distances, indices = nbrs.kneighbors(X_dbscan)\n",
    "# Ordenar as distâncias dos vizinhos mais próximos\n",
    "distances = np.sort(distances[:, min_samples-1])\n",
    "# Plotar o gráfico da distância K-vizinho mais próximol\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Pontos de Dados Ordenados', fontsize=14)\n",
    "plt.ylabel(f'Distância para o {min_samples}º Vizinho Mais Próximo', fontsize=14)\n",
    "plt.title(f'Gráfico K-Vizinho Mais Próximo para Determinar eps (min_samples={min_samples})', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# Testar diferentes valores de eps e min_samples\n",
    "eps_values = [0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.025]  # Exemplo de valores de eps\n",
    "min_samples_values = [3, 5, 7]  # Exemplo de valores de min_samples\n",
    "best_score = -1\n",
    "best_params = None\n",
    "# Iterar sobre combinações de eps e min_samples\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_dbscan)\n",
    "        # Checar se mais de um cluster foi formado\n",
    "        if len(set(labels)) > 1:\n",
    "            # Calcular o score de Silhouette\n",
    "            score = silhouette_score(X_dbscan, labels)\n",
    "            print(f'eps: {eps}, min_samples: {min_samples}, Silhouette Score: {score}')\n",
    "            # Armazenar a melhor configuração de parâmetros\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = (eps, min_samples)\n",
    "print(f'Melhores parâmetros encontrados: eps={best_params[0]}, min_samples={best_params[1]}, Silhouette Score={best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os clusters após aplicar o K-Means\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot dos clusters formados\n",
    "plt.scatter(df_result['total_individual_consumption'], df_result['mean_cond'], \n",
    "            c=df_result['cluster'], cmap='viridis', s=50, label='Clientes')\n",
    "\n",
    "# Plotar os centros dos clusters\n",
    "plt.scatter(cluster_centers_og[:, 0], cluster_centers_og[:, 1], \n",
    "            c='red', s=300, marker='X', edgecolor='black', label='Centros dos Clusters')\n",
    "\n",
    "# Rótulos e título\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('Consumo Total Individual', fontsize=14)\n",
    "plt.ylabel('Média do Consumo do Condomínio', fontsize=14)\n",
    "plt.title('Distribuição dos Clusters de Consumo', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a quantidade de clientes em cada cluster\n",
    "cluster_counts = df_result['cluster'].value_counts()\n",
    "print(\"Quantidade de clientes em cada cluster:\")\n",
    "print(cluster_counts)\n",
    "# Calcular as estatísticas dos clusters\n",
    "cluster_stats = df_result.groupby('cluster').agg({\n",
    "    'total_individual_consumption': ['mean', 'std'],\n",
    "    'mean_cond': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "print(cluster_stats)\n",
    "\n",
    "# Definir um limite para identificar anomalias\n",
    "def identify_anomaly(row, stats):\n",
    "    cluster = row['cluster']\n",
    "    mean_individual = stats.loc[stats['cluster'] == cluster, ('total_individual_consumption', 'mean')].values[0]\n",
    "    std_individual = stats.loc[stats['cluster'] == cluster, ('total_individual_consumption', 'std')].values[0]\n",
    " \n",
    "    if row['total_individual_consumption'] > mean_individual + 2 * std_individual:\n",
    "        return 'Acima da media'\n",
    "    elif row['total_individual_consumption'] < mean_individual - 2 * std_individual:\n",
    "        return 'Abaixo da media'\n",
    "    else:\n",
    "        return 'Normal'\n",
    "# Aplicar a função para identificar anomalias\n",
    "df_result['anomaly'] = df_result.apply(identify_anomaly, axis=1, stats=cluster_stats)\n",
    "\n",
    "print(\"Clientes com possíveis anomalias:\")\n",
    "print(df_result[df_result['anomaly'] != 'Normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "# Configurar Isolation Forest\n",
    "iso_forest = IsolationForest(bootstrap = False, contamination = 0.01, max_features = 1, max_samples = 0.1, n_estimators = 12, random_state=42)  # Contamination define a proporção esperada de anomalias\n",
    "# Aplicar Isolation Forest nos dados de consumo\n",
    "X_iso = df_result[['total_individual_consumption', 'mean_cond']]\n",
    "df_result['iso_forest_anomalia'] = iso_forest.fit_predict(X_iso)\n",
    "# -1 indica anomalias, 1 indica pontos normais\n",
    "df_result['iso_forest_anomalia'] = df_result['iso_forest_anomalia'].map({1: 'Normal', -1: 'Anomalia'})\n",
    "# Visualizar os resultados  \n",
    "print(\"Anomalias detectadas pelo Isolation Forest:\")\n",
    "print(df_result[df_result['iso_forest_anomalia'] == 'Anomalia'])\n",
    "# Visualizar as anomalias detectadas pelo Isolation Forest\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='total_individual_consumption', y='mean_cond', hue='iso_forest_anomalia', data=df_result, palette='coolwarm', s=100)\n",
    "plt.title('Anomalias Detectadas pelo Isolation Forest')\n",
    "plt.xlabel('Consumo Total Individual')\n",
    "plt.ylabel('Média do Consumo do Condomínio')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definir o espaço de busca dos hiperparâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [12, 25, 50, 100, 200, 400, 800],  # Número de árvores\n",
    "    'max_samples': [0.1,0.25,0.5, 0.75, 1.0],  # Fração das amostras\n",
    "    'contamination': np.linspace(start=0.01, stop=0.05, num=10),  # Valores de contaminação\n",
    "    'max_features': [1, 2],  # Número de features\n",
    "    'bootstrap': [False, True],  # Se deve usar bootstrap\n",
    "}\n",
    "\n",
    "# Instanciar o Isolation Forest\n",
    "iso_forest = IsolationForest(random_state=42)\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=iso_forest, param_grid=param_grid, scoring='f1', cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Aplicar o GridSearchCV nos dados\n",
    "X_iso = df_result[['total_individual_consumption', 'mean_cond']]\n",
    "grid_search.fit(X_iso)\n",
    "\n",
    "# Verificar os melhores parâmetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\", best_params)\n",
    "print(\"Melhor pontuação F1 obtida:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Configurar o RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=iso_forest, param_distributions=param_grid, n_iter=50, scoring='f1', cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_iso)\n",
    "\n",
    "# Verificar os melhores parâmetros encontrados\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\", best_params)\n",
    "print(\"Melhor pontuação F1 obtida:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "# Plotar o consumo do outlier\n",
    "plt.scatter(\n",
    "    df3[(df3[\"C_clientCode\"] == 907) & (df3[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df3[(df3[\"C_clientCode\"] == 907) & (df3[\"datetime\"] > \"2024-05\")][\"measure_diff\"],\n",
    "    label=\"Consumo do outlier\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "# Plotar o consumo médio do condomínio\n",
    "plt.scatter(\n",
    "    df3[(df3[\"C_condCode\"] == 27) & (df3[\"datetime\"] > \"2024-05\")][\"datetime\"],\n",
    "    df3[(df3[\"C_condCode\"] == 27) & (df3[\"datetime\"] > \"2024-05\")][\"measure_avg_consumption\"],\n",
    "    label=\"Consumo médio do condomínio\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "# Título e rótulos com tamanho de fonte ajustado\n",
    "plt.title(\"Consumo do outlier vs Consumo do condomínio\", fontsize=20)\n",
    "plt.xlabel(\"Date\", fontsize=20)\n",
    "plt.ylabel(\"Consumption\", fontsize=20)\n",
    "\n",
    "# Configuração dos ticks\n",
    "plt.xticks(rotation=90, fontsize=12)  # Aumentando o tamanho dos valores no eixo X\n",
    "plt.yticks(fontsize=12)  # Aumentando o tamanho dos valores no eixo Y\n",
    "\n",
    "\n",
    "# Adicionar legenda com tamanho de fonte ajustado\n",
    "plt.legend(fontsize=25)\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar os dados do Cluster 1\n",
    "cluster_1 = df_result[df_result['cluster'] == 1]\n",
    "# Exibir as primeiras linhas dos dados do Cluster 1\n",
    "print(\"Clientes pertencentes ao Cluster 1:\")\n",
    "print(cluster_1.head())\n",
    "# Se quiser ver todos os clientes do Cluster 1, pode remover o .head() e usar .to_string() para mostrar todos\n",
    "print(\"Todos os clientes pertencentes ao Cluster 1:\")\n",
    "print(cluster_1.to_string())  # Usar to_string() para mostrar o DataFrame inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera um gráfico de dispersão para clientes pertencentes ao Cluster 1.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='total_individual_consumption', y='mean_cond', data=cluster_1, s=100)\n",
    "plt.title('Clientes pertencentes ao Cluster 1')\n",
    "plt.xlabel('Consumo Total Individual')\n",
    "plt.ylabel('Média do Consumo do Condomínio')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se há valores negativos em 'measure_diff'\n",
    "negativos_measure_diff = df3[df3['measure_diff'] < 0]\n",
    "# Exibir os valores negativos, se existirem\n",
    "if not negativos_measure_diff.empty:\n",
    "    print(\"Valores negativos encontrados em 'measure_diff':\")\n",
    "    print(negativos_measure_diff[['C_clientCode', 'C_condCode', 'measure_diff']].to_string(index=False))\n",
    "else:\n",
    "    print(\"Não há valores negativos em 'measure_diff'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativos_measure_diff['C_clientCode'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativos_measure_diff['C_condCode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['C_condCode'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
